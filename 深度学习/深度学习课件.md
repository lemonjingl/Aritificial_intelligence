![](./imgs/logo.png)





# 第1阶段--人工智能简介

### 1.1 什么是人工智能

> 人工智能(Artificial Intelligence)，是一个以计算机科学（Computer Science）为基础，由计算机、心理学、哲学等多学科交叉融合的交叉学科、新兴学科，研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等

![](./imgs/ai.png)



![](./imgs/ai2.png)





### 1.2 什么是深度学习

深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)

###### 深度学习与机器学习的区别

- 机器学习的特征工程步骤是要靠手动完成的，而且需要大量领域专业知识
- 深度学习通常由多个层组成，它们通常将更简单的模型组合在一起，通过将数据从一层传递到另一层来构建更复杂的模型。通过大量数据的训练自动得到模型，不需要人工设计特征提取环节。
- 深度学习算法试图从数据中学习高级功能，这是深度学习的一个非常独特的部分。因此，减少了为每个问题开发新特征提取器的任务。适合用在难提取特征的图像、语音、自然语言领域



![](./imgs/jiqi.png)



### 1.3 什么是Tensorflow

> TensorFlow™是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库DistBelief [1]  。 Tensorflow拥有多层级结构，可部署于各类服务器、PC终端和网页并支持GPU和TPU高性能数值计算，被广泛应用于谷歌内部的产品开发和各领域的科学研究 [1-2]  。 TensorFlow由谷歌人工智能团队谷歌大脑（Google Brain）开发和维护，拥有包括TensorFlow Hub、TensorFlow Lite、TensorFlow Research Cloud在内的多个项目以及各类应用程序接口（Application Programming Interface, API）  。自2015年11月9日起，TensorFlow依据阿帕奇授权协议（Apache 2.0 open source license）开放源代码 

![](./imgs/tf1.png)

TensorFlow 1 和 TensorFlow 2.x 之间有很多变化。第一个是 Tensorflow.js. 的发布。随着 Web 应用程序越来越占主导地位，在浏览器上部署模型的需求大大增加。借助 Tensorflow.js，你可以使用 Node 在浏览器中运行现有的 python 模型、重新训练现有的模型，并使用 Javascript 完全构建和训练模型（不需要 python）。 Tensorflow 2.x 中的另一个版本是 Tensorflow Lite，一个轻量级库，用于在移动和嵌入式设备上部署模型。这是因为移动和 Web 应用程序是两种最主要的应用程序类型。使用 Tensorflow Lite，你可以简单地将现有模型转换为「compressed flat buffer」，然后将 buffer 加载到移动设备或任何其他嵌入式设备中。这期间发生的主要优化过程是将 32 位浮点值转换成 8 位，这更适合于嵌入式设备（更少的内存使用）。 此外还包括 Tensorflow Extended（TFX）的发布，它是用于部署生产 ML pipeline 的端到端平台。其在机器学习的 3 个最重要领域（web 应用程序、移动应用程序和生产管理）方面做得很好。机器学习生产 pipeline 仍需要大量研究和开发。TFX 可以应对经典的软件生产挑战，例如可扩展性、可维护性和模块化。此外，它还可以帮助解决机器学习的特定挑战，例如持续在线学习、数据验证，数据管理等

### 1.4 机器学习的相关赛事

- **最推荐的两个主流平台——Kaggle和天池**

不管你是萌新还是老手，参加[Kaggle](https://so.csdn.net/so/search?q=Kaggle&spm=1001.2101.3001.7020)和天池平台的比赛总是不错的选择。二者分别是国外和国内最大的机器学习竞赛平台，赛事多，选手多，奖金高，含金量相对充足。

对于新手来说，Kaggle的优势在于开源分享的氛围，每个比赛在讨论区都有大量的针对此比赛的分享，包括Baseline，EDA，Data leakeage等。有些Baseline甚至可以直接进入赛事的Top1%。学习大佬的思路和代码能够少走很多弯路，从而可以飞速地进步。Kaggle唯一的缺点就在于全英文，对于国内的同学来说，理解起来可能有些吃力。

天池是阿里巴巴旗下的机器学习平台，目前是国内最大的机器学习类竞赛网站。和Kaggle相比，开源分享的氛围不是很强。但近期，天池推出了很多针对初学者的入门教程和视频，包括算法讲解和竞赛套路讲解。从天池入门算法竞赛也不失为一个好的选择。

通常来说，这两个平台的参赛队伍数众多，拿Top较难。但正因为大神聚集，所以可以从开源代码里学到很多。

Kaggle：https://www.kaggle.com/

天池：https://tianchi.aliyun.com/home/

------

**2、大厂竞赛**

除了平台赛之外，BATJ等大厂都会定期举办数据/机器学习的旗舰赛事，通常一年一度，奖金非常非常丰厚。如果能够进入Top，更可以直接拿到大厂相关岗位的offer，是非常值得大家关注的系列赛事。但这些赛事都在各自的网站上发布，而且经常会变化，一不小心可能就错过了报名时间

腾讯赛地址：http://tpai.qq.com/race

滴滴赛（Di-tech 2017）：[http://research.xiaojukeji.com](http://research.xiaojukeji.com/)

京东JDD：https://jdder.jd.com/

京东JDATA：https://jdata.jd.com/index.html

蚂蚁金服赛：https://dc.cloud.alipay.com/index

百度点石：https://dianshi.baidu.com/competition

------

**3、国内其他主要平台**

除天池外，国内还有很多其他数据类竞赛的平台。相对天池，参赛的人数相对较少，但某些赛事的奖金也很高，而且这些比赛同样有主办方offer可以拿。因为参与的队伍较少，所以进入Top的几率较大。

DataCastle竞赛平台：

http://www.pkbigdata.com/

Datafountain竞赛平台：

https://www.datafountain.cn/

Biendata平台：

https://www.biendata.com/

Kesci平台：

https://www.kesci.com/

------

**4、国外其他主要平台**

除Kaggle外，国外也有不少相关的机器学习竞赛平台。这些平台各有各的特色，简单介绍两个。

Codalab上多数为学术型的竞赛平台，包括一些学术会议的附赛。第四范式的AutoML赛就在此平台上举办，去年竞赛top3就能免费去NIPS2018会议现场游玩（斯德哥尔摩）。

Crowdai貌似是微软投资，用来对标谷歌旗下的Kaggle平台。虽然赛事较少，但含金量也很不错。

**Codalab：**https://competitions.codalab.org/competitions/

**Crowdai：**https://www.crowdai.org/



# 第2阶段--环境搭建

### 2.1 Tensorflow的编译与安装

######  2.1.1 虚拟环境配置
创建虚拟环境命令
```python
conda create -n deeplearning_env python=3.7
```

```python
进入环境
source activate deeplearning_env
conda activate deeplearning_env
 
退出环境
source deactivate
conda deactivate
列出环境
conda env list
conda info --env
conda info -e
删除环境
conda remove -n deeplearning_env --all
```
导出当前工程依赖的python包清单
```python
pip3 freeze > requirements.txt
pip3 install -r requirements.txt
```

安装: https://tensorflow.google.cn/install

```
pip3 install --user --upgrade tensorflow
```

![](./imgs/tensor1.png)

指定tensorflow版本和指定镜像源头

```
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow-gpu==1.14.0
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow==1.14.0
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple keras==2.2.4
```

###### 2.1.2 mac和window永久设置国内镜像方法

- mac

  ```
  cd ~/    //进入根目录：
  cd .pip  //进入.pip目录
  mkdir .pip  //如果不存在文件夹就新建
  cd .pip  //再次进入.pip目录
  touch pip.conf  //创建pip.conf文件
  vim pip.conf   //修改pip.conf文件
  ```

  修改配置文件：

  ```
  [global]
  index-url=http://mirrors.aliyun.com/pypi/simple/
  [install]
  trusted-host=mirrors.aliyun.com
  ```

  常用的源

  阿里云 http://mirrors.aliyun.com/pypi/simple/

  中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/

  豆瓣(douban) http://pypi.douban.com/simple/

  清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/

- window

  在当前使用用户目录下新建pip文件夹，然后在文件夹中新建一个文本文档， 写入以下内容

  ```
  [global]
  index-url=http://mirrors.aliyun.com/pypi/simple/
  [install]
  trusted-host=mirrors.aliyun.com
  ```

  修改文件名为pip.ini 即可

### 2.2  相关其他依赖模块

在开发深度学习cv项目时候， 我们一般会使用OpenCV， numpy，matplotlib， pandas等模块， 安装方式和上面tensorlow一样

![](./imgs/logo.png)

```
pip install opencv-python
pip install numpy
pip install matplotlib
pip install pandas
```

关于OpenCV和图像我们还需要掌握以下知识才能更好理解后面的图像识别相关操作

###### 2.2.1图像的起源

- 图像是什么

> 图像是人类视觉的基础，是自然景物的客观反映，是人类认识世界和人类本身的重要源泉。“图”是物体反射或透射光的分布，“像“是人的视觉系统所接受的图在人脑中所形版的印象或认识，照片、绘画、剪贴画、地图、书法作品、手写汉学、传真、卫星云图、影视画面、X光片、脑电图、心电图等都是图像

![在这里插入图片描述](https://img-blog.csdnimg.cn/cee2c283c03844a290285cfaea00caf4.png)



###### 2.2.2 模拟图像和数字图像

图像起源于1826年前后法国科学家Joseph Nicéphore Niépce发明的第一张可永久保存的照片，属于模拟图像。模拟图像又称连续图像，它通过某种物理量（如光、电等）的强弱变化来记录图像亮度信息，所以是连续变换的。模拟信号的特点是容易受干扰，如今已经基本全面被数字图像替代。

在第一次世界大战后，1921年美国科学家发明了Bartlane System，并从**伦敦传到纽约传输了第一幅数字图像**，其亮度用离散数值表示，将图片编码成5个灰度级，如下图所示，通过海底电缆进行传输。在发送端图片被编码并使用打孔带记录，通过系统传输后在接收方使用特殊的打印机恢复成图像。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c0ceb385344541ee999464cab1b405a7.png)

1950年左右，计算机被发明，数字图像处理学科正式诞生。

模拟图像和数字图像的对比，我们可以看一下：


![在这里插入图片描述](https://img-blog.csdnimg.cn/b4f15d0fb37248ba842a717ee4e845b0.png)



###### 2.2.3 数字图像的表示

- 位数

计算机采用0/1编码的系统，数字图像也是利用0/1来记录信息，我们平常接触的图像都是8位数图像，包含0～255灰度，其中0，代表最黑，1，表示最白。

![在这里插入图片描述](https://img-blog.csdnimg.cn/4cfd97c6f5aa4b2793247872581ed5a6.png)


人眼对灰度更敏感一些，在16位到32位之间。

![在这里插入图片描述](https://img-blog.csdnimg.cn/515ebd5bdcfa4cc6b20d838e1a91ce37.png)


###### 2.2.4 图像的分类

**二值图像:**

一幅二值图像的二维矩阵仅由0、1两个值构成，“0”代表黑色，“1”代白色。由于每一像素（矩阵中每一元素）取值仅有0、1两种可能，所以计算机中二值图像的数据类型通常为1个二进制位。二值图像通常用于文字、线条图的扫描识别（OCR）和掩膜图像的存储。

**灰度图:**

每个像素只有一个采样颜色的图像，这类图像通常显示为从最暗黑色到最亮的白色的灰度，尽管理论上这个采样可以任何颜色的不同深浅，甚至可以是不同亮度上的不同颜色。灰度图像与黑白图像不同，在计算机图像领域中黑白图像只有黑色与白色两种颜色；但是，灰度图像在黑色与白色之间还有许多级的颜色深度。灰度图像经常是在单个电磁波频谱如可见光内测量每个像素的亮度得到的，用于显示的灰度图像通常用每个采样像素8位的非线性尺度来保存，这样可以有256级灰度（如果用16位，则有65536级）。

**彩色图：**

每个像素通常是由红（R）、绿（G）、蓝（B）三个分量来表示的，分量介于（0，255）。RGB图像与索引图像一样都可以用来表示彩色图像。与索引图像一样，它分别用红（R）、绿（G）、蓝（B）三原色的组合来表示每个像素的颜色。但与索引图像不同的是，RGB图像每一个像素的颜色值（由RGB三原色表示）直接存放在图像矩阵中，由于每一像素的颜色需由R、G、B三个分量来表示，M、N分别表示图像的行列数，三个M x N的二维矩阵分别表示各个像素的R、G、B三个颜色分量。RGB图像的数据类型一般为8位无符号整形，通常用于表示和存放真彩色图像。

###### 2.2.5 OpenCV简介

**OpenCV-Python**

**学习目标**

- 了解OpenCV是什么

- 能够独立安装OpenCV

OpenCV是一款由Intel公司俄罗斯团队发起并参与和维护的一个计算机视觉处理开源软件库，支持与计算机视觉和机器学习相关的众多算法，并且正在日益扩展。

OpenCV的优势：

1. 编程语言

   OpenCV基于C++实现，同时提供python, Ruby, Matlab等语言的接口。OpenCV-Python是OpenCV的Python API，结合了OpenCV C++ API和Python语言的最佳特性。

2. 跨平台

   可以在不同的系统平台上使用，包括Windows，Linux，OS X，Android和iOS。基于CUDA和OpenCL的高速GPU操作接口也在积极开发中

3. 活跃的开发团队

4. 丰富的API

   完善的传统计算机视觉算法，涵盖主流的机器学习算法，同时添加了对深度学习的支持。

###### 2.2.6 OpenCV-Python

OpenCV-Python是一个Python绑定库，旨在解决计算机视觉问题。

Python是一种由Guido van Rossum开发的通用编程语言，它很快就变得非常流行，主要是因为它的简单性和代码可读性。它使程序员能够用更少的代码行表达思想，而不会降低可读性。

与C / C++等语言相比，Python速度较慢。也就是说，Python可以使用C / C++轻松扩展，这使我们可以在C / C++中编写计算密集型代码，并创建可用作Python模块的Python包装器。这给我们带来了两个好处：首先，代码与原始C / C++代码一样快（因为它是在后台工作的实际C++代码），其次，在Python中编写代码比使用C / C++更容易。OpenCV-Python是原始OpenCV C++实现的Python包装器。

OpenCV-Python使用Numpy，这是一个高度优化的数据库操作库，具有MATLAB风格的语法。所有OpenCV数组结构都转换为Numpy数组。这也使得与使用Numpy的其他库（如SciPy和Matplotlib）集成更容易。

###### 2.2.7 OpenCV部署方法

安装OpenCV之前需要先安装numpy, matplotlib。
![在这里插入图片描述](https://img-blog.csdnimg.cn/9d7d4aef808344cbb0824c28adbf1516.png)
创建Python虚拟环境cv, 在cv中安装即可。

先安装OpenCV-Python, 由于一些经典的算法被申请了版权，新版本有很大的限制，所以选用3.4.3以下的版本

```bash
pip install opencv-python==3.4.2.17
```



###### 2.2.8 OpenCV的模块

  下图列出了OpenCV中包含的各个模块：


![在这里插入图片描述](https://img-blog.csdnimg.cn/baa1edbfc30d4f71b8d7e0f5f0fed8ff.png)


其中core、highgui、imgproc是最基础的模块，该课程主要是围绕这几个模块展开的，分别介绍如下：

- **core模块**实现了最核心的数据结构及其基本运算，如绘图函数、数组操作相关函数等。
- **highgui模块**实现了视频与图像的读取、显示、存储等接口。
- **imgproc模块**实现了图像处理的基础方法，包括图像滤波、图像的几何变换、平滑、阈值分割、形态学处理、边缘检测、目标检测、运动分析和对象跟踪等。

对于图像处理其他更高层次的方向及应用，OpenCV也有相关的模块实现

- **features2d模块**用于提取图像特征以及特征匹配，nonfree模块实现了一些专利算法，如sift特征。
- **objdetect模块**实现了一些目标检测的功能，经典的基于Haar、LBP特征的人脸检测，基于HOG的行人、汽车等目标检测，分类器使用Cascade Classification（级联分类）和Latent SVM等。
- **stitching模块**实现了图像拼接功能。
- **FLANN模块**（Fast Library for Approximate Nearest Neighbors），包含快速近似最近邻搜索FLANN
  和聚类Clustering算法。
- **ml模块**机器学习模块（SVM，决策树，Boosting等等）。
- **photo模块**包含图像修复和图像去噪两部分。
- **video模块**针对视频处理，如背景分离，前景检测、对象跟踪等。
- **calib3d模块**即Calibration（校准）3D，这个模块主要是相机校准和三维重建相关的内容。包含了基本的多视角几何算法，单个立体摄像头标定，物体姿态估计，立体相似性算法，3D信息的重建等等。
- **G-API模块**包含超高效的图像处理pipeline引擎




###### 2.2.9 图像的基础操作

**学习目标**

- 掌握图像的读取和保存方法

- 能够使用OpenCV在图像上绘制几何图形

- 能够访问图像的像素

- 能够获取图像的属性，并进行通道的分离和合并

- 能够实现颜色空间的变换

  ------

**图像的IO操作**

这里我们会给大家介绍如何读取图像，如何显示图像和如何保存图像。

**读取图像**

**pycharm**

```
https://www.jetbrains.com/pycharm/download/#section=mac
```



1. API

```python
import cv2 as cv
cv.imread()
```

参数：

- 要读取的图像

- 读取方式的标志

  * cv.IMREAD*COLOR：以彩色模式加载图像，任何图像的透明度都将被忽略。这是默认参数。

  * cv.IMREAD*GRAYSCALE：以灰度模式加载图像

  * cv.IMREAD_UNCHANGED：包括alpha通道的加载图像模式。

    **可以使用1、0或者-1来替代上面三个标志**

2. 参考代码

   ```python
   import numpy as np
   import cv2 as cv
   # 以灰度图的形式读取图像
   img = cv.imread('messi5.jpg',0)
   ```

**注意：如果加载的路径有错误，不会报错，会返回一个None值**

**显示图像**

1 . API

```python
cv.imshow()
```

参数：

- 显示图像的窗口名称，以字符串类型表示
- 要加载的图像

**注意：在调用显示图像的API后，要调用cv.waitKey()给图像绘制留下时间，否则窗口会出现无响应情况，并且图像无法显示出来**。

另外我们也可使用matplotlib对图像进行展示。

2. 参考代码

   ```python
   # opencv中显示
   cv.imshow('image',img)
   cv.waitKey(0)
   # matplotlib中展示
   plt.imshow(img[:,:,::-1])
   ```

**保存图像**

![](./imgs/logo.png)

1. API

   ```python
   cv.imwrite()
   ```

   参数：

   - 文件名，要保存在哪里
   - 要保存的图像

2. 参考代码

   ```python
   cv.imwrite('messigray.png',img)
   ```

   

   我们通过加载灰度图像，显示图像，如果按's'并退出则保存图像，或者按ESC键直接退出而不保存。

   ```python
   import cv2
   
   # 1 读取图像 0是灰度, 1是彩色
   img = cv2.imread("test3.jpg", 1)
   
   # 2 显示图像
   
   cv2.imshow("dili",img)
   cv2.waitKey(0)
   
   cv2.destroyAllWindows()
   
   # # 3 图像保存
   cv2.imwrite('zhangrtianai.png',img)
   
   
   ```

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20be8e0204324170a9b7b9f450c0b2d3.png)

**绘制几何图形**

1. 绘制直线

```
cv.line(img,start,end,color,thickness)
```

参数：

- img:要绘制直线的图像
- Start,end: 直线的起点和终点
- color: 线条的颜色
- Thickness: 线条宽度

2. 绘制圆形

```python
cv.circle(img,centerpoint, r, color, thickness)
```

参数：

- img:要绘制圆形的图像
- Centerpoint, r: 圆心和半径
- color: 线条的颜色
- Thickness: 线条宽度，为-1时生成闭合图案并填充颜色

3. 绘制矩形

```python
cv.rectangle(img,leftupper,rightdown,color,thickness)
```

参数：

- img:要绘制矩形的图像
- Leftupper, rightdown: 矩形的左上角和右下角坐标
- color: 线条的颜色
- Thickness: 线条宽度

4. 向图像中添加文字

```python
cv.putText(img,text,station, font, fontsize,color,thickness,cv.LINE_AA)
```

参数：

- img: 图像
- text：要写入的文本数据
- station：文本的放置位置
- font：字体
- Fontsize :字体大小

**效果展示**

![](./imgs/logo.png)

我们生成一个全黑的图像，然后在里面绘制图像并添加文字

```python
import numpy as np
import cv2
img = np.zeros((1511, 1511, 3), np.uint8)
# 绘制直线  bgr
cv2.line(img, (0, 0), (511, 511), (255, 0, 0), 5)
cv2.line(img, (0, 0), (200, 511), (0, 255, 0), 10)
# 绘制圆形
cv2.circle(img, (256, 256), 100, (0, 0, 255), -1)
# 绘制矩形
cv2.rectangle(img, (100, 100), (400, 400),(0, 255, 0), 8 )
# 绘制文字
cv2.putText(img, "haha", (100, 150), cv2.FONT_HERSHEY_SIMPLEX,5,(255, 0, 0), 3 )
cv2.putText
cv2.imshow('huizhi', img)
cv2.waitKey(0)

cv2.destroyAllWindows()

```

结果：




###### 2.2.10 获取并修改图像中的像素点

我们可以通过行和列的坐标值获取该像素点的像素值。对于BGR图像，它返回一个蓝，绿，红值的数组。对于灰度图像，仅返回相应的强度值。使用相同的方法对像素值进行修改。

```python
import numpy as np
import cv2 as cv
img = cv.imread('messi5.jpg')
# 获取某个像素点的值
px = img[100,100]
# 仅获取蓝色通道的强度值
blue = img[100,100,0]
# 修改某个位置的像素值
img[100,100] = [255,255,255]
```



###### 2.2.11 算数操作

**学习目标**

- 了解图像的加法、混合操作

 **图像的加法**

你可以使用OpenCV的cv.add()函数把两幅图像相加，或者可以简单地通过numpy操作添加两个图像，如res = img1 + img2。两个图像应该具有相同的大小和类型，或者第二个图像可以是标量值。

**注意：OpenCV加法和Numpy加法之间存在差异。OpenCV的加法是饱和操作，而Numpy添加是模运算。**

![](./imgs/logo.png)

参考以下代码：

```python
>>> x = np.uint8([250])
>>> y = np.uint8([10])
>>> print( cv.add(x,y) ) # 250+10 = 260 => 255
[[255]]
>>> print( x+y )          # 250+10 = 260 % 256 = 4
[4]
```

这种差别在你对两幅图像进行加法时会更加明显。OpenCV 的结果会更好一点。所以我们尽量使用 OpenCV 中的函数。

我们将下面两幅图像：


![在这里插入图片描述](https://img-blog.csdnimg.cn/689fedef35264394adbd6da3eec1f20c.png)

![](./imgs/logo.png)

代码：

```python
import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt

# 1 读取图像
img1 = cv.imread("view.jpg")
img2 = cv.imread("rain.jpg")

# 2 加法操作
img3 = cv.add(img1,img2) # cv中的加法
img4 = img1+img2 # 直接相加

# 3 图像显示
fig,axes=plt.subplots(nrows=1,ncols=2,figsize=(10,8),dpi=100)
axes[0].imshow(img3[:,:,::-1])
axes[0].set_title("cv中的加法")
axes[1].imshow(img4[:,:,::-1])
axes[1].set_title("直接相加")
plt.show()
```

结果如下所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/1f434921900e4064b7b0935377823c57.png)

###### 2.2.12 图像的混合

这其实也是加法，但是不同的是两幅图像的权重不同，这就会给人一种混合或者透明的感觉。图像混合的计算公式如下：

> g(x) = (1−α)f0(x) + αf1(x)

通过修改 α 的值（0 → 1），可以实现非常炫酷的混合。

现在我们把两幅图混合在一起。第一幅图的权重是0.7，第二幅图的权重是0.3。函数cv2.addWeighted()可以按下面的公式对图片进行混合操作。

> dst = α⋅img1 + β⋅img2 + γ

这里γ取为零。

![](./imgs/logo.png)

参考以下代码：

```python
import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt

# 1 读取图像
img1 = cv.imread("view.jpg")
img2 = cv.imread("rain.jpg")

# 2 图像混合
img3 = cv.addWeighted(img1,0.7,img2,0.3,0)

# 3 图像显示
plt.figure(figsize=(8,8))
plt.imshow(img3[:,:,::-1])
plt.show()
```

窗口将如下图显示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/626d56504c6f4617897bf2907b0a69ac.png)

###### 2.2.13 视频读写

**从文件中读取视频并播放**

在OpenCV中我们要获取一个视频，需要创建一个VideoCapture对象，指定你要读取的视频文件：

1. 创建读取视频的对象

   ```python
   cap = cv.VideoCapture(filepath)
   ```

   参数：

   - filepath: 视频文件路径

2. 视频的属性信息

   2.1.  获取视频的某些属性，

   ```python
   retval = cap.get(propId)
   ```

   参数：

   - propId: 从0到18的数字，每个数字表示视频的属性

     常用属性有：

    ![在这里插入图片描述](https://img-blog.csdnimg.cn/0a9087b9970b45ea9b104e3be556884e.png)


   2.2 修改视频的属性信息

   ```python
cap.set(propId，value) 
   ```

   参数：

   - proid: 属性的索引，与上面的表格相对应
   - value: 修改后的属性值

3. 判断图像是否读取成功

   ```python
   isornot = cap.isOpened()
   ```

   - 若读取成功则返回true，否则返回False

4. 获取视频的一帧图像

   ```python
   ret, frame = cap.read()
   ```

   参数：

   - ret: 若获取成功返回True，获取失败，返回False
   - Frame: 获取到的某一帧的图像

5. 调用cv.imshow()显示图像，在显示图像时使用cv.waitkey()设置适当的持续时间，如果太低视频会播放的非常快，如果太高就会播放的非常慢，通常情况下我们设置25ms就可以了。

6. 最后，调用cap.realease()将视频释放掉



![](./imgs/logo.png)

示例：

```python
import numpy as np
import cv2 as cv
# 1.获取视频对象
cap = cv.VideoCapture('DOG.wmv')
# 2.判断是否读取成功
while(cap.isOpened()):
    # 3.获取每一帧图像
    ret, frame = cap.read()
    # 4. 获取成功显示图像
    if ret == True:
        cv.imshow('frame',frame)
    # 5.每一帧间隔为25ms
    if cv.waitKey(25) & 0xFF == ord('q'):
        break
# 6.释放视频对象
cap.release()
cv.destoryAllwindows()
```

###### 2.2.14 保存视频

在OpenCV中我们保存视频使用的是VedioWriter对象，在其中指定输出文件的名称，如下所示：

1. 创建视频写入的对象

```python
out = cv2.VideoWriter(filename,fourcc, fps, frameSize)
```

参数：

- filename：视频保存的位置
- fourcc：指定视频编解码器的4字节代码  
- fps：帧率
- frameSize：帧大小

2. 设置视频的编解码器，如下所示，

   ```
   retval = cv2.VideoWriter_fourcc( c1, c2, c3, c4 )
   ```

   参数：

   - c1,c2,c3,c4: 是视频编解码器的4字节代码，在[fourcc.org](http://www.fourcc.org/codecs.php)中找到可用代码列表，与平台紧密相关，常用的有：

     ###### 在Windows中：DIVX（.avi）

     ###### 在OS中：MJPG（.mp4），DIVX（.avi），X264（.mkv）。

3. 利用cap.read()获取视频中的每一帧图像，并使用out.write()将某一帧图像写入视频中。

4. 使用cap.release()和out.release()释放资源。

![](./imgs/logo.png)

示例：

```python
import cv2 as cv
import numpy as np

# 1. 读取视频
cap = cv.VideoCapture("DOG.wmv")

# 2. 获取图像的属性（宽和高，）,并将其转换为整数
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

# 3. 创建保存视频的对象，设置编码格式，帧率，图像的宽高等
out = cv.VideoWriter('outpy.avi',cv.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
while(True):
    # 4.获取视频中的每一帧图像
    ret, frame = cap.read()
    if ret == True: 
        # 5.将每一帧图像写入到输出文件中
        out.write(frame)
    else:
        break 

# 6.释放资源
cap.release()
out.release()
cv.destroyAllWindows()
```

# 

### 2.3   Tensorflow实现SoftMaxRegression识别手写数字

###### 2.3.1神经网络

人工神经网络（ Artificial Neural Network， 简写为ANN）也简称为神经网络（NN）。是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）结构和功能的 计算模型。经典的神经网络结构包含三个层次的神经网络。分别输入层，输出层以及隐藏层。
![在这里插入图片描述](https://img-blog.csdnimg.cn/efd46ebebf0d4d8d81d5caa38fbd43eb.png)
其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出，输入层的神经元只是输入。

神经网络的特点
- 每个连接都有个权值
- 同一层神经元之间没有连接
- 最后的输出结果对应的层也称之为`全连接层`FC

那么为什么设计这样的结构呢？首先从一个最基础的结构说起，神经元。以前也称之为感知机。神经元就是要模拟人的神经元结构。
![在这里插入图片描述](https://img-blog.csdnimg.cn/5156ddc76d0740f08bea4f91f25a456c.png)
1943 年，McCulloch 和 Pitts 将上述情形抽象为上图所示的简单模型，这就是一直沿用至今的 M-P 神经元模型。把许多这样的神经元按一定的层次结构连接起来，就得到了神经网络。一个简单的神经元如下图所示，
![在这里插入图片描述](https://img-blog.csdnimg.cn/c02b33d4e0c64339a4c5ec845a7203f2.png)
其中:


![在这里插入图片描述](https://img-blog.csdnimg.cn/e02d1108ddb14ead8df2085667814ae3.png)
可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。

######  2.3.3 感知机(PLA: Perceptron Learning Algorithm))
感知机就是模拟这样的大脑神经网络处理数据的过程。感知机模型如下图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/040a3e1ab1c241dc941c43acd4e489d5.png)
感知机是一种最基础的分类模型，类似于逻辑回归。感知机最基础是这样的函数，而逻辑回归用的sigmoid。这个感知机具有连接的权重和偏置
![在这里插入图片描述](https://img-blog.csdnimg.cn/d7be1807cec14417a63821d1d3215b5d.png)
感知机的激活函数是符号函数：sign(z) = +1 (if z >=0) else -1


感知机的作用：

把一个n维向量空间用一个超平面分割成两部分，给定一个输入向量，超平面可以判断出这个向量位于超平面的哪一边，得到输入时正类或者是反类，对应到2维空间就是一条直线把一个平面分为两个部分。

- 单层神经网络
是最基本的神经元网络形式，由有限个神经元构成，所有神经元的输入向量都是同一个向量。由于每一个神经元都会产生一个标量结果，所以单层神经元的输出是一个向量，向量的维数等于神经元的数目。示意图如下
![在这里插入图片描述](https://img-blog.csdnimg.cn/e6bf0bb574d54953a6856d1e49973f6d.png)
###### 2.3.4 多层神经网络
多层神经网络就是由单层神经网络进行叠加之后得到的，所以就形成了层的概念，常见的多层神经网络有如下结构：

- 输入层（Input layer），众多神经元（Neuron）接受大量输入消息。输入的消息称为输入向量。
- 输出层（Output layer），消息在神经元链接中传输、分析、权衡，形成输出结果。输出的消息称为输出向量。
- 隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。隐层可以有一层或多层。隐层的节点（神经元）数目不定，但数目越多神经网络的非线性越显著，从而神经网络的强健性（robustness）更显著。
示意图如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/69a9bad4a5e64c3395211d5cd19f5136.png)
全连接层

全连接层：当前一层和前一层每个神经元相互链接，我们称当前这一层为全连接层。
![在这里插入图片描述](https://img-blog.csdnimg.cn/48a82d46fa824214b37d10794d6b934a.png)
从上图可以看出，所谓的全连接层就是在前一层的输出的基础上进行一次
![在这里插入图片描述](https://img-blog.csdnimg.cn/32159c34d1064ee3bfa0cb2fb3f543fc.png)
的变化(不考虑激活函数的情况下就是一次线性变化，所谓线性变化就是平移(+b)和缩放的组合(*w))
 - 激活函数

在前面的神经元的介绍过程中我们提到了激活函数，那么他到底是干什么的呢？

假设我们有这样一组数据，三角形和四边形，需要把他们分为两类

![在这里插入图片描述](https://img-blog.csdnimg.cn/854a5fae468a461b81c95c2ae1c0b5b2.png)



通过不带激活函数的感知机模型我们可以划出一条线, 把平面分割开

![在这里插入图片描述](https://img-blog.csdnimg.cn/0a6142042ade48cbaeaa58e9005d40a5.png)


假设我们确定了参数w和b之后，那么带入需要预测的数据，如果y>0,我们认为这个点在直线的右边，也就是正类（三角形），否则是在左边（四边形）

但是可以看出，三角形和四边形是没有办法通过直线分开的，那么这个时候该怎么办？

可以考虑使用多层神经网络来进行尝试，比如**在前面的感知机模型中再增加一层**
![在这里插入图片描述](https://img-blog.csdnimg.cn/54baf25f3064461896cd05a008405baf.png)



对上图中的等式进行合并，我们可以得到：

![在这里插入图片描述](https://img-blog.csdnimg.cn/ff0147fb806140c38f135dbfd0d7bb57.png)


上式括号中的都为w参数，和公式
![在这里插入图片描述](https://img-blog.csdnimg.cn/ea124915e75447ff8737e1e9dc097771.png)

完全相同，依然只能够绘制出直线

所以可以发现，即使是多层神经网络，相比于前面的感知机，没有任何的改进。

但是如果此时，我们在前面感知机的基础上加上**非线性的激活函数**之后，输出的结果就不在是一条直线

![在这里插入图片描述](https://img-blog.csdnimg.cn/30101e7d716e448181e2f92df5b1e7cb.png)



如上图，右边是sigmoid函数，对感知机的结果，通过sigmoid函数进行处理

如果给定合适的参数w和b，就可以得到合适的曲线，能够完成对最开始问题的非线性分割

所以激活函数很重要的一个**作用**就是**增加模型的非线性分割能力**

常见的激活函数有：

![在这里插入图片描述](https://img-blog.csdnimg.cn/80eeeef1bb0a42da81bd6b5a70ce5357.png)



看图可知：

- sigmoid 只会输出正数，以及靠近0的输出变化率最大
- tanh和sigmoid不同的是，tanh输出可以是负数
- Relu是输入只能大于0,如果你输入含有负数，Relu就不适合，如果你的输入是图片格式，Relu就挺常用的，因为图片的像素值作为输入时取值为[0,255]。

激活函数的作用除了前面说的**增加模型的非线性分割能力**外，还有

- **提高模型鲁棒性**
- **缓解梯度消失问题**
- **加速模型收敛等**

这些好处，大家后续会慢慢体会到，这里先知道就行



###### 2.3.5 神经网络示例

一个男孩想要找一个女朋友，于是实现了一个**女友判定机**，随着年龄的增长，他的判定机也一直在变化

14岁的时候：

![在这里插入图片描述](https://img-blog.csdnimg.cn/4ab1d9f52cbb463983a5d2d4abb0761e.png)


无数次碰壁之后，男孩意识到追到女孩的可能性和颜值一样重要，于是修改了判定机：
![在这里插入图片描述](https://img-blog.csdnimg.cn/f3e6a53db600426c9389ec2e77e46d98.png)


在15岁的时候终于找到呢女朋友，但是一顿时间后他发现有各种难以忍受的习惯，最终决定分手。一段空窗期中，他发现找女朋友很复杂，需要更多的条件才能够帮助他找到女朋友，于是在25岁的时候，他再次修改了判定机：

![在这里插入图片描述](https://img-blog.csdnimg.cn/18a5242148b44e79a61e7d66cd551194.png)



在更新了女友判定机之后，问题又来了，很多指标不能够很好的量化，如何颜值，什么样的叫做颜值高，什么样的叫做性格好等等，为了解决这个问题，他又更新了判定机，最终得到**超级女友判定机**

![在这里插入图片描述](https://img-blog.csdnimg.cn/4779ca8cab0a401eba4559efeabd2c00.png)
上述的超级女友判定机其实就是神经网络，它能够接受基础的输入，通过隐藏层的线性的和非线性的变化最终的到输出

通过上面例子，希望大家能够理解深度学习的**思想**：

输出的最原始、最基本的数据，通过模型来进行特征工程，进行更加高级特征的学习，然后通过传入的数据来确定合适的参数，让模型去更好的拟合数据。

这个过程可以理解为盲人摸象，多个人一起摸，把摸到的结果乘上合适的权重，进行合适的变化，让他和目标值趋近一致。整个过程只需要输入基础的数据，程序自动寻找合适的参数。


###### 2.3.6 playground使用和演示
网址：http://playground.tensorflow.org

神经网络的主要用途在于分类，那么整个神经网络分类的原理是怎么样的？我们还是围绕着损失、优化这两块去说。神经网络输出结果如何分类？
![在这里插入图片描述](https://img-blog.csdnimg.cn/4a23e65549024732b3ed3d17aa6f287a.png)
 神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。

任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率的和为1）。如果将分类问题中“一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax回归就是一个非常常用的方法。
###### 2.3.7 softmax回归
![在这里插入图片描述](https://img-blog.csdnimg.cn/b9d8096ac357486abc37b4110c144b12.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/bd84e64e6a1e47dc9bd48766734dbd76.png)
###### 2.3.8 softmax特点

如何理解这个公式的作用呢？看一下计算案例

Softmax回归将神经网络输出转换成概率结果

```
假设输出结果为：2.3, 4.1, 5.6
softmax的计算输出结果为：
y1_p = e^2.3/(e^2.3+e^4.1+e^5.6)
y1_p = e^4.1/(e^2.3+e^4.1+e^5.6)
y1_p = e^5.6/(e^2.3+e^4.1+e^5.6)
```
这样就把神经网络的输出也变成了一个概率输出
![在这里插入图片描述](https://img-blog.csdnimg.cn/3070c1dc8a4a4bea8e173458a097eb11.png)
 类似于逻辑回归当中的sigmoid函数，sigmoid输出的是某个类别的概率

想一想线性回归的损失函数以及逻辑回归的损失函数，那么如何去衡量神经网络预测的概率分布和真实答案的概率分布之间的距离？
###### 2.3.9 交叉熵损失
![在这里插入图片描述](https://img-blog.csdnimg.cn/9b18600cafaf451ca408bffb5ea499ec.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210714085507199.png)
为了能够衡量距离，目标值需要进行one-hot编码，能与概率值一一对应，如下图

![在这里插入图片描述](https://img-blog.csdnimg.cn/437def96f9724e07a9dc8aab74662cb1.png)
它的损失如何计算？
```
0log(0.10)+0log(0.05)+0log(0.15)+0log(0.10)+0log(0.05)+0log(0.20)+1log(0.10)+0log(0.05)+0log(0.10)+0log(0.10)
```
上述的结果为1log(0.10)，那么为了减少这一个样本的损失。神经网络应该怎么做？所以会提高对应目标值为1的位置输出概率大小，由于softmax公式影响，其它的概率必定会减少。只要这样进行调整这样是不是就预测成功了！！！！！

 损失大小

神经网络最后的损失为平均每个样本的损失大小。对所有样本的损失求和取其平均值
有了这两个关键部分，神经网络的分类就是这样去做的，但是它是如何优化这些输出概率的呢?

###### 2.3.10 BP算法(了解)(Backpropagation)

神经网络当中充满大量的权重、偏置参数，这些参数都需要去进行优化。之前我们接触的线性回归、逻辑回归通过梯度下降优化参数。这里也是一样，只不过由于神经网络的隐层可以增加很多层，那么这个过程需要一种规则。

    定义：梯度下降+链式求导规则

BP算法过程

1、前向传输（Feed-Forward）

从输入层=>隐藏层=>输出层，一层一层的计算所有神经元输出值的过程。

2、逆向反馈（Back Propagation）

因为输出层的值与真实的值会存在误差，我们可以用均方误差来衡量预测值和真实值之间的误差。

- 在手工设定了神经网络的层数，每层的神经元的个数，学习率 η（下面会提到）后，BP 算法会先随机初始化每条连接线权重和偏置
- 对于训练集中的每个输入 x 和输出 y，BP 算法都会先执行前向传输得到预测值
- 根据真实值与预测值之间的误差执行逆向反馈更新神经网络中每条连接线的权重和每层的偏好。

我们不会详细地讨论可以如何使用反向传播和梯度下降等算法训练参数。训练过程中的计算机会尝试一点点增大或减小每个参数，看其能如何减少相比于训练数据集的误差，以望能找到最优的权重、偏置参数组合
![在这里插入图片描述](https://img-blog.csdnimg.cn/a8dfdf6e933743f29704348f34161d64.png)

###### 2.3.11 案例: Pytorch+线性神经网络进行波士顿房价预测
- Pytorch下载
  https://pytorch.org/get-started/locally/ 选择对应的版本命令
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/2c205492823249b981865a3ec73368b3.png)

  ![](./imgs/logo.png)
- 加载数据

```python
import torch
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y < 50.0]
y = y[y < 50.0]
X_train, X_test, y_train, y_test = train_test_split(X, y)
standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train = standardScaler.transform(X_train)
X_test = standardScaler.transform(X_test)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```
((367, 13), (123, 13), (367,), (123,))

- 训练
```python
#net
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, 100)
        self.predict = torch.nn.Linear(100, n_output)
    def forward(self, x):
        out = self.hidden(x)
        out = torch.relu(out)
        out = self.predict(out)
        return out
```
```python
net = Net(13, 1)
#loss
loss_func = torch.nn.MSELoss()
#optimiter
optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
#training
for i in range(10000):
    x_data = torch.tensor(X_train, dtype=torch.float32)
    y_data = torch.tensor(y_train, dtype=torch.float32)
    pred = net.forward(x_data)
   #   squeeze(a)就是将a中所有为1的维度删掉
    pred = torch.squeeze(pred)
    loss = loss_func(pred, y_data) * 0.001

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print("ite:{}, loss_train:{}".format(i, loss))
    print(pred[0:10])
    print(y_data[0:10])

    #test
    x_data = torch.tensor(X_test, dtype=torch.float32)
    y_data = torch.tensor(y_test, dtype=torch.float32)
    pred = net.forward(x_data)
    pred = torch.squeeze(pred)
    loss_test = loss_func(pred, y_data) * 0.001
    print("ite:{}, loss_test:{}".format(i, loss_test))

torch.save(net, "boston_model.pkl")
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/9295f59272fe4268b401872033a5eb2f.png)

- 加载模型测试

  ![](./imgs/logo.png)
```python
net = torch.load("boston_model.pkl")
loss_func = torch.nn.MSELoss()
#test
x_data = torch.tensor(X_test, dtype=torch.float32)
y_data = torch.tensor(y_test, dtype=torch.float32)
pred = net.forward(x_data)
pred = torch.squeeze(pred)
loss_test = loss_func(pred, y_data) * 0.001
print("loss_test:{}".format(loss_test))
```
loss_test:0.015603514388203621




###### 2.3.12 使用KreasAPI搭建神经网络
安装: https://tensorflow.google.cn/install
```
pip3 install --user --upgrade tensorflow
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/01bd94fd9c4342a28c4ca2086e3ee254.png)
Keras是一个由Python编写的开源人工神经网络库，可以作为Tensorflow、Microsoft-CNTK和Theano的高阶应用程序接口，进行深度学习模型的设计、调试、评估、应用和可视化
![在这里插入图片描述](https://img-blog.csdnimg.cn/33be614a86944f308d0cf90eee0e2fc6.png)


```python
#建立一个Sequential顺序模型
from keras.models import Sequential
model = Sequential()
```

    Using TensorFlow backend.

![](./imgs/logo.png)

```python
#通过.add()叠加各层网络
# Dense即全连接层，逻辑上等价于这样一个函数：权重W为m*n的矩阵.输入x为n维向量.
# 激活函数Activation.偏置bias.输出向量out为m维向量.out=Activation(Wx+bias).
# 即一个线性变化加一个非线性变化产生输出.这是深度神经网络非常基本又十分强大的结构。
# 它的能力可以参考万能近似定理.很多其他的层，比如卷积层，都是在全连接的基础上加了很多先验形成的
from keras.layers import Dense
model.add(Dense(units=3, activation='sigmoid', input_dim=3))
model.add(Dense(units=1, activation='sigmoid'))
```

    WARNING:tensorflow:From C:\Users\Eric\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.

- API 介绍

dense(
    inputs,
    units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)

- inputs: 输入数据，2维tensor. 
- units: 该层的神经单元结点数。 
- activation: 激活函数. 
- use_bias: Boolean型，是否使用偏置项. 
- kernel_initializer: 卷积核的初始化器. 
- bias_initializer: 偏置项的初始化器，默认初始化为0. 
- kernel_regularizer: 卷积核化的正则化，可选. 
- bias_regularizer: 偏置项的正则化，可选. 
- activity_regularizer: 输出的正则化函数. 
- trainable: Boolean型，表明该层的参数是否参与训练。如果为真则变量加入到图集合中 GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). 
- name: 层的名字. 
- reuse: Boolean型, 是否重复使用参数.
- 全连接层执行操作 outputs = activation(inputs.kernel+bias) 如果执行结果不想进行激活操作，则设置activation=None


```python
#查看模型结构
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_1 (Dense)              (None, 3)                 12        
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 4         
    =================================================================
    Total params: 16
    Trainable params: 16
    Non-trainable params: 0
    _________________________________________________________________

![](./imgs/logo.png)

```python
#通过.compile()配置模型求解过程参数
# 交叉熵损失函数 categorical_crossentropy
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
```


```python
#训练模型
# model.fit(x_train, y_train, epochs=5)
```

###### 2.3.13 Tenfsorflow 构建多层感知机
好坏质检二分类mlp实战
1、通过mlp模型，在不增加特征项的情况下，实现了非线性二分类任务；
2、掌握了mlp模型的建立、配置与训练方法，并实现基于新数据的预测；
3、熟悉了mlp分类的预测数据格式，并实现格式转换；
4、核心算法参考链接：https://keras-cn.readthedocs.io/en/latest/#30sker
基于data.csv (链接：https://pan.baidu.com/s/1GoWnSt8ArNxa75mhys8Dnw 
提取码：8888 
--来自百度网盘超级会员V5的分享)数据，建立mlp模型，计算其在测试数据上的准确率，可视化模型预测结果：

1.进行数据分离：test_size=0.33,random_state=10
2.模型结构：一层隐藏层，有20个神经元

```python
#loada the data
import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0323</td>
      <td>0.0244</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0887</td>
      <td>0.0244</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1690</td>
      <td>0.0163</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.2420</td>
      <td>0.0000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2420</td>
      <td>0.0488</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
#define the X and y
X = data.drop(['y'],axis=1)
y = data.loc[:,'y']
X.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0323</td>
      <td>0.0244</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0887</td>
      <td>0.0244</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1690</td>
      <td>0.0163</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.2420</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2420</td>
      <td>0.0488</td>
    </tr>
  </tbody>
</table>
</div>

![](./imgs/logo.png)


```python
#visualize the data
%matplotlib inline
from matplotlib import pyplot as plt
fig1 = plt.figure(figsize=(5,5))
passed=plt.scatter(X.loc[:,'x1'][y==1],X.loc[:,'x2'][y==1])
failed=plt.scatter(X.loc[:,'x1'][y==0],X.loc[:,'x2'][y==0])
plt.legend((passed,failed),('passed','failed'))
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('raw data')
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/505855c4082d4e97a5598ca77c458cf3.png)


```python
#split the data
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=10)
print(X_train.shape,X_test.shape,X.shape)
```

    (275, 2) (136, 2) (411, 2)

![](./imgs/logo.png)

```python
#set up the model
from keras.models import Sequential
from keras.layers import Dense, Activation

mlp = Sequential()
mlp.add(Dense(units=20, input_dim=2, activation='sigmoid'))
mlp.add(Dense(units=1,activation='sigmoid'))
mlp.summary()
```

    Using TensorFlow backend.


    WARNING:tensorflow:From C:\Users\Eric\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_1 (Dense)              (None, 20)                60        
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 21        
    =================================================================
    Total params: 81
    Trainable params: 81
    Non-trainable params: 0
    _________________________________________________________________

![](./imgs/logo.png)

```python
#compile the model
# binary_crossentropy经常搭配sigmoid分类函数，categorical_crossentropy搭配softmax分类函数
# binary_crossentropy只能二分类
mlp.compile(optimizer='adam',loss='binary_crossentropy')
```


```python
#train the model
mlp.fit(X_train,y_train,epochs=3000)
```

    WARNING:tensorflow:From C:\Users\Eric\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/3000
    275/275 [==============================] - 1s 2ms/step - loss: 0.6932
    Epoch 2/3000
    275/275 [==============================] - 0s 94us/step - loss: 0.6927
    Epoch 3/3000
    275/275 [==============================] - 0s 94us/step - loss: 0.6926
    Epoch 4/3000
    275/275 [==============================] - 0s 94us/step - loss: 0.6925
    Epoch 5/3000
    275/275 [==============================] - 0s 98us/step - loss: 0.6925
    Epoch 6/3000
    275/275 [==============================] - 0s 83us/step - loss: 0.6926
    Epoch 7/3000
    275/275 [==============================] - 0s 91us/step - loss: 0.6924
    Epoch 8/3000
    275/275 [==============================] - 0s 101us/step - loss: 0.6923
    Epoch 9/3000
    275/275 [==============================] - 0s 94us/step - loss: 0.6924
    Epoch 10/3000


![在这里插入图片描述](https://img-blog.csdnimg.cn/0e47541f062043488ead01fb5275ae68.png)




```python
#make prediction and calculate the accuracy
y_train_predict = mlp.predict_classes(X_train)
from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(y_train,y_train_predict)
print(accuracy_train)
```

    0.9781818181818182



```python
#make prediction based on the test data
y_test_predict = mlp.predict_classes(X_test)
accuracy_test = accuracy_score(y_test,y_test_predict)
print(accuracy_test)
```

    0.9779411764705882



```python
print(y_train_predict[0:10])
#y_train_predict_form = pd.Series(i[0] for i in y_train_predict)

#print(y_train_predict_form)

```

    [[1]
     [0]
     [0]
     [0]
     [1]
     [0]
     [1]
     [1]
     [1]
     [1]]



```python
#generate new data for plot
xx, yy = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))
x_range = np.c_[xx.ravel(),yy.ravel()]
y_range_predict = mlp.predict_classes(x_range)
print(type(y_range_predict))
```

    <class 'numpy.ndarray'>



```python
#format the output
y_range_predict_form = pd.Series(i[0] for i in y_range_predict)

print(y_range_predict_form)
```

    0       1
    1       1
    2       1
    3       1
    4       1
           ..
    9995    1
    9996    1
    9997    1
    9998    1
    9999    1
    Length: 10000, dtype: int64

![](./imgs/logo.png)

```python
fig2 = plt.figure(figsize=(5,5))
passed_predict=plt.scatter(x_range[:,0][y_range_predict_form==1],x_range[:,1][y_range_predict_form==1])
failed_predict=plt.scatter(x_range[:,0][y_range_predict_form==0],x_range[:,1][y_range_predict_form==0])

passed=plt.scatter(X.loc[:,'x1'][y==1],X.loc[:,'x2'][y==1])
failed=plt.scatter(X.loc[:,'x1'][y==0],X.loc[:,'x2'][y==0])
plt.legend((passed,failed,passed_predict,failed_predict),('passed','failed','passed_predict','failed_predict'))
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('prediction result')
plt.show()
```


![在这里插入图片描述](https://img-blog.csdnimg.cn/74f8c05e5d3841a68745e8ef00ce541a.png)

好坏质检二分类mlp实战summary：
1、通过mlp模型，在不增加特征项的情况下，实现了非线性二分类任务；
2、掌握了mlp模型的建立、配置与训练方法，并实现基于新数据的预测；
3、熟悉了mlp分类的预测数据格式，并实现格式转换；
4、核心算法参考链接：https://keras-cn.readthedocs.io/en/latest/#30skeras
###### 2.3.14 Tensorflow进行手写数字识别
基于mnist数据集，建立mlp模型，实现0-9数字的十分类task：：

1.实现mnist数据载入，可视化图形数字
2.完成数据预处理：图像数据维度转换与归一化、输出结果格式转换
3.计算模型在预测数据集的准确率
4.模型结构：两层隐藏层，每层有392个神经元

```python
# load the mnist data
from keras.datasets import mnist
(X_train,y_train),(X_test,y_test) = mnist.load_data()
```

    Using TensorFlow backend.



```python
print(type(X_train),X_train.shape)
```

    <class 'numpy.ndarray'> (60000, 28, 28)

![](./imgs/logo.png)

```python
#visualize the data
img1 = X_train[0]
%matplotlib inline
from matplotlib import pyplot as plt
fig1 = plt.figure(figsize=(3,3))
plt.imshow(img1)
plt.title('image size: 28 X 28')
plt.show()
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/89429c7237714818938850f9f415bd01.png)



```python
img1
```
    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,
             18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,
            253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,
            253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,
            253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,
            205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,
             90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,
            190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,
            253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,
            241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,
            148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,
            253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,
            253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,
            195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,
             11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0]], dtype=uint8)




```python
#format the input data
feature_size = img1.shape[0]*img1.shape[1]
X_train_format = X_train.reshape(X_train.shape[0],feature_size)
X_test_format = X_test.reshape(X_test.shape[0],feature_size)


print(X_train_format.shape)
```

    (60000, 784)



```python
#normalize the input data
X_train_normal = X_train_format/255
X_test_normal = X_test_format/255
print(X_train_normal[0])
```

    [0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.01176471 0.07058824 0.07058824 0.07058824
     0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.
     0.96862745 0.49803922 0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.11764706 0.14117647 0.36862745 0.60392157
     0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686
     0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.19215686
     0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686
     0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863
     0.32156863 0.21960784 0.15294118 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.07058824 0.85882353 0.99215686
     0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549
     0.96862745 0.94509804 0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.31372549 0.61176471 0.41960784 0.99215686
     0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.54509804 0.99215686 0.74509804 0.00784314 0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.04313725
     0.74509804 0.99215686 0.2745098  0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.1372549  0.94509804
     0.88235294 0.62745098 0.42352941 0.00392157 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.31764706 0.94117647 0.99215686
     0.99215686 0.46666667 0.09803922 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.17647059 0.72941176 0.99215686 0.99215686
     0.58823529 0.10588235 0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.97647059 0.99215686 0.97647059 0.25098039 0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.18039216 0.50980392 0.71764706 0.99215686
     0.99215686 0.81176471 0.00784314 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.15294118 0.58039216
     0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686
     0.99215686 0.78823529 0.30588235 0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.09019608 0.25882353 0.83529412 0.99215686
     0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.07058824 0.67058824
     0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588
     0.31372549 0.03529412 0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686
     0.99215686 0.95686275 0.52156863 0.04313725 0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.53333333 0.99215686
     0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.         0.         0.
     0.         0.         0.         0.        ]

![](./imgs/logo.png)

```python
# 对标签进行one hot编码 独热
# from tensorflow.keras.utils import to_categorical
y_train = tf.one_hot(indices=y_train, depth=10, on_value=1, off_value=0,axis=1)
y_test = tf.one_hot(indices=y_test, depth=10, on_value=1, off_value=0,axis=1)
# 显示one hot的值
sess=tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(y_train[1]))

 # 用这个 对标签进行one hot编码 独热
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
```


```python
print(X_train_normal.shape,y_train_format.shape)
```

    (60000, 784) (60000, 10)


![在这里插入图片描述](https://img-blog.csdnimg.cn/2380931ceddb4f02bf6cdfd0d3c99ab0.png)




```python
#set up the model
from keras.models import Sequential
from keras.layers import Dense, Activation

mlp = Sequential()
mlp.add(Dense(units=392,activation='relu',input_dim=784))
mlp.add(Dense(units=392,activation='relu'))
mlp.add(Dense(units=10,activation='softmax'))
mlp.summary()
```

    WARNING:tensorflow:From C:\Users\Eric\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_1 (Dense)              (None, 392)               307720    
    _________________________________________________________________
    dense_2 (Dense)              (None, 392)               154056    
    _________________________________________________________________
    dense_3 (Dense)              (None, 10)                3930      
    =================================================================
    Total params: 465,706
    Trainable params: 465,706
    Non-trainable params: 0
    _________________________________________________________________



```python
#configure the model
mlp.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['categorical_accuracy'])
```


```python
#train the model
mlp.fit(X_train_normal,y_train_format,epochs=10)
```

    WARNING:tensorflow:From C:\Users\Eric\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/10
    60000/60000 [==============================] - 51s 857us/step - loss: 0.1879 - categorical_accuracy: 0.9431 51s - loss: 0.3920 - categorical_accur - ETA: 50s - loss: 0.3800 - categori - ETA: 41s - loss: 0.3229 - categorical_accuracy:  - ETA: 41s - 
    Epoch 2/10
    60000/60000 [==============================] - 53s 877us/step - loss: 0.0825 - categorical_accuracy: 0.9742 40s - lo - ETA: 39s - loss: 0.0823 - categor - ETA: 37s - loss: 0.0828 - categorical_accuracy - ETA: 36s - loss - ETA: 34s - loss: 0.0859 - categorical_accuracy: 0.973 - ETA: 34s - loss: 0.0856 - categorical_accuracy: 0 - ETA: 33s - loss: 0.0853 - categorical_accuracy:  - ETA: 3 - ETA: 3 - ETA - ETA: 18s - los
    Epoch 3/10
    60000/60000 [==============================] - 46s 759us/step - loss: 0.0553 - categorical_accuracy: 0.9824s - loss: 0.0557 - catego
    Epoch 4/10
    60000/60000 [==============================] - 40s 674us/step - loss: 0.0432 - categorical_accuracy: 0.9863 49s - loss: 0.0286  - ETA: 47s - loss: 0.0310 - ETA: 39s - loss: 0.0333 - categorical_ac - - ETA: 1s - loss: 0.042
    Epoch 5/10
    60000/60000 [==============================] - 33s 546us/step - loss: 0.0343 - categorical_accuracy: 0.9889 26s - loss: 0.0253 - categorical_accuracy: 0.992 - - ETA: 11s - loss: 0.0298 - categorical_accuracy: - ETA: 8s - los - - ETA: 2s - ETA: 1s -
    Epoch 6/10
    60000/60000 [==============================] - 33s 547us/step - loss: 0.0286 - categorical_accuracy: 0.9909s - loss: 0.0272 -  - ETA: 0s - loss: 0.0287 - categorica
    Epoch 7/10
    60000/60000 [==============================] - 34s 565us/step - loss: 0.0242 - categorical_accuracy: 0.9927 30s - loss: 0.0186 - categorical_accuracy: 0 - ETA: 30s - loss: 0.0187 - categorical_accuracy: 0.993 - ETA: 30s -
    Epoch 8/10
    60000/60000 [==============================] - 33s 557us/step - loss: 0.0226 - categorical_accuracy: 0.9930
    Epoch 9/10
    60000/60000 [==============================] - 33s 547us/step - loss: 0.0175 - categorical_accuracy: 0.9945 29s - loss: 0 - ETA: 22s - loss: 0.0106 - ETA: 20s - loss: 0.0126 - categorical_accuracy: 0. - ETA: 19s - - ETA: 17s - loss: 0.0152 - categor - ETA: 13s - loss: 0.0168 - categorical_accuracy: 0.994 - ETA: 12s - loss: 0.0168 -  - ET - ETA: 9s - ETA: 1s - loss: - ETA: 0s - loss: 0.0175 - categorical_accuracy: 0.99
    Epoch 10/10
    60000/60000 [==============================] - 33s 548us/step - loss: 0.0179 - categorical_accuracy: 0.9941s - loss: 0.0178 - categorical_accuracy: 0.99 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.0178 - 



![](./imgs/logo.png)

    <keras.callbacks.History at 0x1b539bbae48>




```python
#evaluate the model
y_train_predict = mlp.predict_classes(X_train_normal)
print(type(y_train_predict))

```

    <class 'numpy.ndarray'>



```python
print(y_train_predict[0:10])
```

    [5 0 4 1 9 2 1 3 1 4]



```python
from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(y_train,y_train_predict)
print(accuracy_train)
```

    0.9948166666666667



```python
y_test_predict = mlp.predict_classes(X_test_normal)
accuracy_test = accuracy_score(y_test,y_test_predict)
print(accuracy_test)
```

    0.9799



```python
img2 = X_test[100]
fig2 = plt.figure(figsize=(3,3))
plt.imshow(img2)
plt.title(y_test_predict[100])
plt.show()
```





```python
# coding:utf-8
import matplotlib as mlp
font2 = {'family' : 'SimHei',
'weight' : 'normal',
'size'   : 20,
}
mlp.rcParams['font.family'] = 'SimHei'
mlp.rcParams['axes.unicode_minus'] = False
a = [i for i in range(1,10)]
fig4 = plt.figure(figsize=(5,5))
for i in a:
    plt.subplot(3,3,i)
    plt.tight_layout()
    plt.imshow(X_test[i])
    plt.title('predict:{}'.format(y_test_predict[i]),font2)
    plt.xticks([])
    plt.yticks([])
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/8abd014f48104b10babc5d370c03852a.png)

- 完整代码

  ![](./imgs/logo.png)

```python
# 加载数据
from keras.datasets import mnist
import cv2 as cv
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from matplotlib import pyplot as plt
import numpy as np
from tensorflow.keras.utils import to_categorical
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(type(x_train))

def show():
    cv.imshow('mnsit', x_train[1])
    # print(x_train[1])
    # print(y_train[1])
    cv.waitKey(0)
    cv.destroyAllWindows()
# 整合
x_train = x_train.reshape(x_train.shape[0],28*28)
x_test = x_test.reshape(x_test.shape[0],28*28)

print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

# 归一化
x_train = x_train/255
x_test = x_test/255
# 对 y 进行独热编码
# y_train = tf.one_hot(indices=y_train, depth=10, on_value=1, off_value=0, axis=1)
y_train = to_categorical(y_train, 10)
# y_test = to_categorical(y_test, 10)
# y_test = tf.one_hot(indices=y_test, depth=10, on_value=1, off_value=0, axis=1)
# print(type(y_train[0]))
#
# sess=tf.Session()
# sess.run(tf.global_variables_initializer())
# print(sess.run(y_train[1]))
print(y_train[0])
# 深度学习模型
model = Sequential()
model.add(Dense(units=400, input_dim=784, activation='relu'))
model.add(Dense(units=400, activation='relu'))
# fc
model.add(Dense(units=10, activation='softmax'))
model.summary()

model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=200)
# model.fit(x_train, y_train, epochs=10)

# 评价模型
y_pred = model.predict_classes(x_test)
print(y_pred[:2])


print(f"accuracy = {sum(y_pred == y_test)/len(y_test)}")


a = [i for i in range(1,10)]
fig4 = plt.figure(figsize=(5,5))
for i in a:
    plt.subplot(3,3,i)
    plt.tight_layout()
    plt.imshow(x_test[i].reshape(28, 28))
    plt.title('predict:{}'.format(y_pred[i]))

plt.show()
```

图像数字多分类实战summary：
1、通过mlp模型，实现了基于图像数据的数字自动识别分类；
2、完成了图像的数字化处理与可视化；
3、对mlp模型的输入、输出数据格式有了更深的认识，完成了数据预处理与格式转换；
4、建立了结构更为复杂的mlp模型
5、mnist数据集地址：http://yann.lecun.com/exdb/mnist/



# 第3阶段--Tensorflow实现自编码器及多层感知机

### 3.1 自编码器简介

自编码器
自编码器是一种神经网络，它的输入输出是一致的，目标是使用稀疏的高阶特征重新组合起来重构自己。自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是和数据相关的、有损的、从样本中自动学习的。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。

**设计自编码器**

- 搭建自编码器
-  搭建解码器
- 设定损失函数

![](./imgs/bianmaqi1.png)

###### 3.1.1 编码器的应用

- 数据去噪
- 降维
- 生成图像

###### 3.1.2 变分自编码器(VAE)

可以随机生成隐含变量，提高网络的[泛化](https://so.csdn.net/so/search?q=泛化&spm=1001.2101.3001.7020)能力，比普通的自动编码器更好，缺点就是生成的图片会有点模糊。

### 3.2 Tensorflow实现自编码器

![](./imgs/logo.png)

**代码详解**

- 导入相关包

```
from keras.datasets import mnist
import tensorflow as tf
print(tf.__version__)
from keras.models import Sequential
from keras.layers import Dense
from matplotlib import pyplot as plt
import numpy as np
from tensorflow.keras.utils import to_categorical
```

- 导入准备好的数据集

```
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(type(x_train))
# 整合
x_train = x_train.reshape(x_train.shape[0],28*28)
x_test = x_test.reshape(x_test.shape[0],28*28)
```

- 图片的归一化操作

```
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
input_size=784
hidden_size=32
output_size=784

# 归一化
x_train = x_train/255
x_test = x_test/255
# 对 y 进行独热编码
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
```

- 设置好输入输出的图片大小

```
input=tf.keras.layers.Input(shape=(input_size,))

```

- 建立好编码器和解码器

```
en = tf.keras.layers.Dense(hidden_size,activation='relu')(input)
de = tf.keras.layers.Dense(output_size,activation='sigmoid')(en)
```

- 将建立好的model进行编译和运行

```
model = tf.keras.Model(inputs=input,outputs=de)
model.compile(optimizer='adam',loss='mse')
model.fit(
    x_train,x_train,epochs=5,batch_size=256,shuffle=True,validation_data=(x_test,x_test)
)
```

- 将解码器"取出"

```
encode = tf.keras.Model(inputs=input,outputs=en)
input_de = tf.keras.layers.Input(shape=(hidden_size,))
output_de = model.layers[-1](input_de)

decode = tf.keras.Model(inputs=input_de,outputs=output_de)

```

- 构造测试数据

```
encode_test = encode(x_test)
decode_test= decode(encode_test)
```

- 测试

```
plt.figure(figsize=(20, 4))
for i in range(1, n):
    ax = plt.subplot(2, n, i)
    plt.imshow(np.array(x_test[i]).reshape(28, -1))

    ax = plt.subplot(2, n, i + n)
    print(decode_test[i])


plt.show()
```

- 最终结果

  ```
  # 加载数据
  from keras.datasets import mnist
  import tensorflow as tf
  print(tf.__version__)
  from keras.models import Sequential
  from keras.layers import Dense
  from matplotlib import pyplot as plt
  import numpy as np
  from tensorflow.keras.utils import to_categorical
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  print(type(x_train))
  # 整合
  x_train = x_train.reshape(x_train.shape[0],28*28)
  x_test = x_test.reshape(x_test.shape[0],28*28)
  
  print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
  input_size=784
  hidden_size=32
  output_size=784
  
  # 归一化
  x_train = x_train/255
  x_test = x_test/255
  # 对 y 进行独热编码
  y_train = to_categorical(y_train, 10)
  y_test = to_categorical(y_test, 10)
  input=tf.keras.layers.Input(shape=(input_size,))
  #encode
  en = tf.keras.layers.Dense(hidden_size,activation='relu')(input)
  
  #decode
  de = tf.keras.layers.Dense(output_size,activation='sigmoid')(en)
  
  
  model = tf.keras.Model(inputs=input,outputs=de)
  model.compile(optimizer='adam',loss='mse')
  model.fit(
      x_train,x_train,epochs=5,batch_size=256,shuffle=True,validation_data=(x_test,x_test)
  )
  encode = tf.keras.Model(inputs=input,outputs=en)
  input_de = tf.keras.layers.Input(shape=(hidden_size,))
  output_de = model.layers[-1](input_de)
  
  decode = tf.keras.Model(inputs=input_de,outputs=output_de)
  
  
  encode_test = encode(x_test)
  decode_test= decode(encode_test)
  
  n = 10
  
  plt.figure(figsize=(20, 4))
  for i in range(1, n):
      ax = plt.subplot(2, n, i)
      plt.imshow(np.array(x_test[i]).reshape(28, -1))
  
      ax = plt.subplot(2, n, i + n)
      print(decode_test[i])
  
  
  plt.show()
  ```

### 3.3 多层感知机

###### 3.3.1 感知机是什么

感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流 / 不流”（1/0）两种取值。0 对应“不传递信号”，1 对应“传递信号”。

图 2-1 是一个接收两个输入信号的感知机的例子。_x_1、_x_2 是输入信号，_y_ 是输出信号，_w_1、_w_2 是权重（_w_ 是 weight 的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（_w_1_x_1、_w_2_x_2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出 1。这也称为“神经元被激活”。这里将这个界限值称为**阈值**，用符号 _θ_ 表示。

![{70%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAwNi5wbmc?x-oss-process=image/format,png)

**图 2-1　有两个输入的感知机**

感知机的运行原理只有这些！把上述内容用数学式来表示，就是式（2.1）。

![{65%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzE4Mi5wbmc?x-oss-process=image/format,png)

感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。

###### 3.3.2 简单逻辑电路

- 与门

现在让我们考虑用感知机来解决简单的问题。这里首先以逻辑电路为题材来思考一下与门（AND gate）。与门是有两个输入和一个输出的门电路。图 2-2 这种输入信号和输出信号的对应表称为“真值表”。如图 2-2 所示，与门仅在两个输入均为 1 时输出 1，其他时候则输出 0。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAwNy5wbmc?x-oss-process=image/format,png)

**图 2-2　与门的真值表**

下面考虑用感知机来表示这个与门。需要做的就是确定能满足图 2-2 的真值表的 _w_1、_w_2、_θ_ 的值。那么，设定什么样的值才能制作出满足图 2-2 的条件的感知机呢？

实际上，满足图 2-2 的条件的参数的选择方法有无数多个。比如，当 (_w_1,_w_2,_θ_)=(0.5,0.5,0.7)时，可以满足图 2-2 的条件。此外，当(_w_1,_w_2,_θ_)为 (0.5, 0.5, 0.8) 或者 (1.0, 1.0, 1.0) 时，同样也满足与门的条件。设定这样的参数后，仅当 _x_1 和 _x_2 同时为 1 时，信号的加权总和才会超过给定的阈值 _θ_。

- 与非门和或门

接着，我们再来考虑一下与非门（NAND gate）。NAND 是 Not AND 的意思，与非门就是颠倒了与门的输出。用真值表表示的话，如图 2-3 所示，仅当 _x_1 和 _x_2 同时为 1 时输出 0，其他时候则输出 1。那么与非门的参数又可以是什么样的组合呢？

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAwOC5wbmc?x-oss-process=image/format,png)

**图 2-3　与非门的真值表**

要表示与非门，可以用(_w_1,_w_2,_θ_)=(-0.5,-0.5,-0.7)这样的组合（其他的组合也是无限存在的）。实际上，只要把实现与门的参数值的符号取反，就可以实现与非门。

接下来看一下图 2-4 所示的或门。或门是“只要有一个输入信号是 1，输出就为 1”的逻辑电路。那么我们来思考一下，应该为这个或门设定什么样的参数呢？

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAwOS5wbmc?x-oss-process=image/format,png)

**图 2-4　或门的真值表**

> 这里决定感知机参数的并不是计算机，而是我们人。我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。**学习**是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。

如上所示，我们已经知道使用感知机可以表示与门、与非门、或门的逻辑电路。这里重要的一点是：与门、与非门、或门的感知机构造是一样的。实际上，3 个门电路只有参数的值（权重和阈值）不同。也就是说，相同构造的感知机，只需通过适当地调整参数的值，就可以像“变色龙演员”表演不同的角色一样，变身为与门、与非门、或门。

###### 3.3.3 感知机的实现

简单的实现

现在，我们用 Python 来实现刚才的逻辑电路。这里，先定义一个接收参数 `x1` 和 `x2` 的 `AND` 函数。

    def AND(x1, x2):
        w1, w2, theta = 0.5, 0.5, 0.7
        tmp = x1*w1 + x2*w2
        if tmp <= theta:
            return 0
        elif tmp > theta:
            return 1


在函数内初始化参数 `w1`、`w2`、`theta`，当输入的加权总和超过阈值时返回 `1`，否则返回 `0`。我们来确认一下输出结果是否如图 2-2 所示。

    AND(0, 0) # 输出0
    AND(1, 0) # 输出0
    AND(0, 1) # 输出0
    AND(1, 1) # 输出1


果然和我们预想的输出一样！这样我们就实现了与门。按照同样的步骤，也可以实现与非门和或门，不过让我们来对它们的实现稍作修改。

- 导入权重和偏置

刚才的与门的实现比较直接、容易理解，但是考虑到以后的事情，我们将其修改为另外一种实现形式。在此之前，首先把式（2.1）的 _θ_ 换成 -_b_，于是就可以用式（2.2）来表示感知机的行为。

![{65%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzE4My5wbmc?x-oss-process=image/format,png)

式（2.1）和式（2.2）虽然有一个符号不同，但表达的内容是完全相同的。此处，_b_ 称为**偏置**，_w_1 和 _w_2 称为**权重**。如式（2.2）所示，感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于 0 则输出 1，否则输出 0。下面，我们使用 NumPy，按式（2.2）的方式实现感知机。在这个过程中，我们用 Python 的解释器逐一确认结果。
```
>>> import numpy as np
>>> x = np.array([0, 1])     # 输入
>>> w = np.array([0.5, 0.5]) # 权重
>>> b = -0.7                 # 偏置
>>> w*x
array([ 0. , 0.5])
>>> np.sum(w*x)
0.5
>>> np.sum(w*x) + b
-0.19999999999999996   # 大约为-0.2（由浮点小数造成的运算误差）
```
如上例所示，在 NumPy 数组的乘法运算中，当两个数组的元素个数相同时，各个元素分别相乘，因此 `w*x` 的结果就是它们的各个元素分别相乘（`[0, 1] * [0.5, 0.5] => [0, 0.5]`）。之后，`np.sum(w*x)` 再计算相乘后的各个元素的总和。最后再把偏置加到这个加权总和上，就完成了式（2.2）的计算。

- 使用权重和偏置的实现

使用权重和偏置，可以像下面这样实现与门。

    def AND(x1, x2):
        x = np.array([x1, x2])
        w = np.array([0.5, 0.5])
        b = -0.7
        tmp = np.sum(w*x) + b
        if tmp <= 0:
           return 0
        else:
           return 1


这里把 -_θ_ 命名为偏置 _b_，但是请注意，偏置和权重 _w_1、_w_2 的作用是不一样的。具体地说，_w_1 和 _w_2 是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度（输出信号为 1 的程度）的参数。比如，若 _b_ 为 -0.1，则只要输入信号的加权总和超过 0.1，神经元就会被激活。但是如果 _b_ 为 -20.0，则输入信号的加权总和必须超过 20.0，神经元才会被激活。像这样，偏置的值决定了神经元被激活的容易程度。另外，这里我们将 _w_1 和 _w_2 称为权重，将 _b_ 称为偏置，但是根据上下文，有时也会将 _b_、_w_1、_w_2 这些参数统称为权重。

接着，我们继续实现与非门和或门。
```
def NAND(x1, x2):
    x = np.array(\[x1, x2\])
    **w = np.array(\[-0.5, -0.5\])** # 仅权重和偏置与AND不同！
    **b = 0.7**
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

def OR(x1, x2):
    x = np.array(\[x1, x2\])
    **w = np.array(\[0.5, 0.5\])** # 仅权重和偏置与AND不同！
    **b = -0.2**
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```
与门、与非门、或门是具有相同构造的感知机，区别只在于权重参数的值。因此，在与非门和或门的实现中，仅设置权重和偏置的值这一点和与门的实现不同。

###### 3.3.4 感知机的局限性

到这里我们已经知道，使用感知机可以实现与门、与非门、或门三种逻辑电路。现在我们来考虑一下异或门（XOR gate）。

- 异或门

异或门也被称为**逻辑异或**电路。如图 2-5 所示，仅当 _x_1 或 _x_2 中的一方为 1 时，才会输出 1（“异或”是拒绝其他的意思）。那么，要用感知机实现这个异或门的话，应该设定什么样的权重参数呢？

![{65%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxMC5wbmc?x-oss-process=image/format,png)

**图 2-5　异或门的真值表**

实际上，用前面介绍的感知机是无法实现这个异或门的。为什么用感知机可以实现与门、或门，却无法实现异或门呢？下面我们尝试通过画图来思考其中的原因。

首先，我们试着将或门的动作形象化。或门的情况下，当权重参数(_b_,_w_1,_w_2)=(-0.5,1.0,1.0)时，可满足图 2-4 的真值表条件。此时，感知机可用下面的式（2.3）表示。

![{65%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzE4NC5wbmc?x-oss-process=image/format,png)

式（2.3）表示的感知机会生成由直线-0.5+_x_1+_x_2=0 分割开的两个空间。其中一个空间输出 1，另一个空间输出 0，如图 2-6 所示。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxMS5wbmc?x-oss-process=image/format,png)

**图 2-6　感知机的可视化：灰色区域是感知机输出 0 的区域，这个区域与或门的性质一致**

或门在(_x_1,_x_2)=(0,0)时输出 0，在 (_x_1,_x_2) 为 (0, 1)、(1, 0)、(1, 1) 时输出 1。图 2-6 中，○ 表示 0，△ 表示 1。如果想制作或门，需要用直线将图 2-6 中的○和△分开。实际上，刚才的那条直线就将这 4 个点正确地分开了。

那么，换成异或门的话会如何呢？能否像或门那样，用一条直线作出分割图 2-7 中的○和△的空间呢？

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxMi5wbmc?x-oss-process=image/format,png)

**图 2-7　○ 和 △ 表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？**

想要用一条直线将图 2-7 中的○和△分开，无论如何都做不到。事实上，用一条直线是无法将○和△分开的。

- 线性和非线性

图 2-7 中的○和△无法用一条直线分开，但是如果将“直线”这个限制条件去掉，就可以实现了。比如，我们可以像图 2-8 那样，作出分开○和△的空间。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxMy5wbmc?x-oss-process=image/format,png)

**图 2-8　使用曲线可以分开○和△**

感知机的局限性就在于它只能表示由一条直线分割的空间。图 2-8 这样弯曲的曲线无法用感知机表示。另外，由图 2-8 这样的曲线分割而成的空间称为**非线性**空间，由直线分割而成的空间称为**线性**空间。线性、非线性这两个术语在机器学习领域很常见，可以将其想象成图 2-6 和图 2-8 所示的直线和曲线。

###### 3.3.5 多层感知机

感知机不能表示异或门让人深感遗憾，但也无需悲观。实际上，感知机的绝妙之处在于它可以“叠加层”（通过叠加层来表示异或门是本节的要点）。这里，我们暂且不考虑叠加层具体是指什么，先从其他视角来思考一下异或门的问题。

- 已有门电路的组合

异或门的制作方法有很多，其中之一就是组合我们前面做好的与门、与非门、或门进行配置。这里，与门、与非门、或门用图 2-9 中的符号表示。另外，图 2-9 中与非门前端的○表示反转输出的意思。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxNC5wbmc?x-oss-process=image/format,png)

**图 2-9　与门、与非门、或门的符号**

那么，请思考一下，要实现异或门的话，需要如何配置与门、与非门和或门呢？这里给大家一个提示，用与门、与非门、或门代替图 2-10 中的各个“？”，就可以实现异或门。

![{80%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxNS5wbmc?x-oss-process=image/format,png)

**图 2-10　将与门、与非门、或门代入到“？”中，就可以实现异或门！**

> 2.4 节讲到的感知机的局限性，严格地讲，应该是“单层感知机无法表示异或门”或者“单层感知机无法分离非线性空间”。接下来，我们将看到通过组合感知机（叠加层）就可以实现异或门。

异或门可以通过图 2-11 所示的配置来实现。这里，_x_1 和 _x_2 表示输入信号，_y_ 表示输出信号。_x_1 和 _x_2 是与非门和或门的输入，而与非门和或门的输出则是与门的输入。

![{80%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxNi5wbmc?x-oss-process=image/format,png)

**图 2-11　通过组合与门、与非门、或门实现异或门**

现在，我们来确认一下图 2-11 的配置是否真正实现了异或门。这里，把 _s_1 作为与非门的输出，把 _s_2 作为或门的输出，填入真值表中。结果如图 2-12 所示，观察 _x_1、_x_2 、_y_，可以发现确实符合异或门的输出。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxNy5wbmc?x-oss-process=image/format,png)

**图 2-12　异或门的真值表**

异或门的实现

下面我们试着用 Python 来实现图 2-11 所示的异或门。使用之前定义的 AND 函数、NAND 函数、OR 函数，可以像下面这样（轻松地）实现。

    def XOR(x1, x2):
        s1 = NAND(x1, x2)
        s2 = OR(x1, x2)
        y = AND(s1, s2)
        return y


这个 `XOR` 函数会输出预期的结果。

    XOR(0, 0) # 输出0
    XOR(1, 0) # 输出1
    XOR(0, 1) # 输出1
    XOR(1, 1) # 输出0


这样，异或门的实现就完成了。下面我们试着用感知机的表示方法（明确地显示神经元）来表示这个异或门，结果如图 2-13 所示。

![{85%}](https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5pdHVyaW5nLmNvbS5jbi9maWd1cmVzLzIwMTgvRGVlcExlYXJuaW5nLzAxOC5wbmc?x-oss-process=image/format,png)

**图 2-13　用感知机表示异或门**

如图 2-13 所示，异或门是一种多层结构的神经网络。这里，将最左边的一列称为第 0 层，中间的一列称为第 1 层，最右边的一列称为第 2 层。

图 2-13 所示的感知机与前面介绍的与门、或门的感知机（图 2-1）形状不同。实际上，与门、或门是单层感知机，而异或门是 2 层感知机。叠加了多层的感知机也称为**多层感知机**（multi-layered perceptron）。

在图 2-13 所示的 2 层感知机中，先在第 0 层和第 1 层的神经元之间进行信号的传送和接收，然后在第 1 层和第 2 层之间进行信号的传送和接收，具体如下所示。

> 1.  第 0 层的两个神经元接收输入信号，并将信号发送至第 1 层的神经元。
> 2.  第 1 层的神经元将信号发送至第 2 层的神经元，第 2 层的神经元输出 _y_。

这种 2 层感知机的运行过程可以比作流水线的组装作业。第 1 段（第 1 层）的工人对传送过来的零件进行加工，完成后再传送给第 2 段（第 2 层）的工人。第 2 层的工人对第 1 层的工人传过来的零件进行加工，完成这个零件后出货（输出）。

像这样，在异或门的感知机中，工人之间不断进行零件的传送。通过这样的结构（2 层结构），感知机得以实现异或门。这可以解释为“单层感知机无法表示的东西，通过增加一层就可以解决”。也就是说，通过叠加层（加深层），感知机能进行更加灵活的表示。



# 第4阶段--前向神经网络

### 4.1 基本概念

从输入到输出的过程中不存在于模型自身的反馈连接的模型，此类模型被称为“前馈”。
 深度前馈网络是一类网络模型的统称，有多层感知机、自编码器、限制玻尔兹曼机，以及卷积神经网络等。

1、多层感知机与布尔函数

1. 多层感知机表示异或逻辑时最少需要几个隐藏层(仅考虑二元输入)？
    **一个隐藏层即可计算异或函数**。
    0个隐藏层的情况(等同于逻辑回归)无法精确学习出一个输出为异或的模型表示。事实上，**通用近似定理**告诉我们，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，当给予网络**足够数量的隐藏单元**时，可以以任意精度近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数。我们常用的激活函数和目标函数是通用近似定理使用的一个子集，因此多层感知机的表达能力是非常强的，关键是我们能否学习到对应此表达式的模型参数。
2. 如果只使用一个隐藏层，需要多少隐节点能够实现包含![n](https://math.jianshu.com/math?formula=n)元输入的任意布尔函数？
    ![2^{n-1}](https://math.jianshu.com/math?formula=2%5E%7Bn-1%7D)个隐节点。
3. 考虑多隐层的情况，实现包含![n](https://math.jianshu.com/math?formula=n)元输入的任意布尔函数最少需要多少个网络节点和网络层？
    ![n](https://math.jianshu.com/math?formula=n)元异或函数需要包括![3(n-1)](https://math.jianshu.com/math?formula=3(n-1))个节点(包括最终输出节点)。可以发现，多隐层结构可以将隐节点的数目从指数级![O(2^{n-1})](https://math.jianshu.com/math?formula=O(2%5E%7Bn-1%7D))直接减少至线性级![O(3(n-1))](https://math.jianshu.com/math?formula=O(3(n-1)))。
    需要的最少网络层数为![2\log_2N](https://math.jianshu.com/math?formula=2%5Clog_2N)(向上取整)。

2、深度神经网络中的激活函数

1. 线性模型是机器学习领域中最基本也是最重要的工具，以逻辑回归和线性回归为例，无论通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合数据。但我们往往会遇到**线性不可分问题**(如XOR异或函数)，需要**非线性变换**对数据分布进行重新映射，对于深度神经网络，我们在每一层线性变换后叠加一个**非线性激活函数**，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。

2. 常用激活函数及其导数：
    **Sigmoid激活函数**的形式为：![f(z)=\frac{1}{1+exp(-z)},](https://math.jianshu.com/math?formula=f(z)%3D%5Cfrac%7B1%7D%7B1%2Bexp(-z)%7D%2C) ![f'(z)=f(z)(1-f(z)).](https://math.jianshu.com/math?formula=f'(z)%3Df(z)(1-f(z)).)**Tanh激活函数**的形式为：![f(z)={\rm tanh}(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}},](https://math.jianshu.com/math?formula=f(z)%3D%7B%5Crm%20tanh%7D(z)%3D%5Cfrac%7Be%5Ez-e%5E%7B-z%7D%7D%7Be%5Ez%2Be%5E%7B-z%7D%7D%2C) ![f'(z)=1-(f(z))^2.](https://math.jianshu.com/math?formula=f'(z)%3D1-(f(z))%5E2.)**ReLU激活函数**的形式为：![f(z)=\max(0,z),](https://math.jianshu.com/math?formula=f(z)%3D%5Cmax(0%2Cz)%2C) ![f'(z)=\begin{cases} 1, &z>0; \\ 0, &z\leq0. \end{cases}](https://math.jianshu.com/math?formula=f'(z)%3D%5Cbegin%7Bcases%7D%201%2C%20%26z%3E0%3B%20%5C%5C%200%2C%20%26z%5Cleq0.%20%5Cend%7Bcases%7D)

3. Sigmoid和Tanh激活函数为什么会导致梯度消失的现象？

   Sigmoid和Tanh激活函数曲线如下图所示：

   ![img](https:////upload-images.jianshu.io/upload_images/15697855-138613a46b7b3cef.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

   **激活函数**

   Sigmoid函数的导数在很大或很小时都会趋近于0，造成梯度消失，同样Tanh函数的导数在很大或很小时都会趋近于0，同样造成梯度消失。实际上Tanh激活函数相当于Sigmoid的平移：

   

4. ReLU系列激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？

- 优点：
   (1) 从计算的角度上，Sigmoid和Tanh激活函数都**需要计算指数函数**，**复杂度高**，而ReLU**只需要一个阈值**即可得到激活值。
   (2) ReLU的**非饱和性**可以有效解决梯度消失的问题，提供**相对宽的激活边界**。
   (3) ReLU的**单侧抑制**提供了网络的**稀疏表达能力**。
- 局限性：
   其训练过程中会**导致神经元死亡**。函数![f(z)=\max(0,z)](https://math.jianshu.com/math?formula=f(z)%3D%5Cmax(0%2Cz))导致负梯度在经过ReLU单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。
   为了解决这个问题，人们设计了Leaky ReLU(LReLU)，其形式表达式为![f(z)=\begin{cases} z, &z>0; \\ az, &z\leq0. \end{cases}](https://math.jianshu.com/math?formula=f(z)%3D%5Cbegin%7Bcases%7D%20z%2C%20%26z%3E0%3B%20%5C%5C%20az%2C%20%26z%5Cleq0.%20%5Cend%7Bcases%7D)
   一般![a](https://math.jianshu.com/math?formula=a)为一个**很小的正常数**，LReLU既**实现了单侧抑制**，又**保留了部分负梯度**信息以致信息不完全丢失。但是另一方面，**![a](https://math.jianshu.com/math?formula=a)值的选择**增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。
   基于此，参数化的**PReLU**(Parametric ReLU)将负轴部分斜率![a](https://math.jianshu.com/math?formula=a)作为网络中一个**可学习的参数**，进行反向传播训练，与其他含参数网络层联合优化。
   **RReLU**(Random ReLU)增加了“随机化”机制，在训练过程中，斜率![a](https://math.jianshu.com/math?formula=a)作为一个满足某种分布的随机采样；测试时再固定下来，一定程度上能起到**正则化**的作用。

多层感知机的反向传播算法

1. 在网络训练中，前向传播最终产生一个标量损失函数，反向传播算法则将损失函数的信息沿网络层向后传播用以计算梯度，达到优化网络参数的目的。
2. 多层感知机的平方误差和交叉熵损失函数的计算公式就不写了，之前已经详细说过
3. **平方误差损失**函数和**交叉熵损失**函数分别适用的场景：
    平方损失函数：更适合输出为**连续**，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；
    交叉熵损失函数：更适合**二分类**或**多分类**的场景。
4. 为什么平方损失不适合最后一层含有Sigmoid或Softmax激活函数的神经网络？
    平方误差损失函数相对于输出层的导数![\sigma^{(L)}=-(y-a^{(L)})f'(z^{(L)}),](https://math.jianshu.com/math?formula=%5Csigma%5E%7B(L)%7D%3D-(y-a%5E%7B(L)%7D)f'(z%5E%7B(L)%7D)%2C)其中最后一项![f'(z^{(L)})](https://math.jianshu.com/math?formula=f'(z%5E%7B(L)%7D))为激活函数的导数，当激活函数为Sigmoid函数时，如果![z^{(L)}](https://math.jianshu.com/math?formula=z%5E%7B(L)%7D)的绝对值较大，函数的梯度会**趋于饱和**，即![f'(z^{(L)})](https://math.jianshu.com/math?formula=f'(z%5E%7B(L)%7D))的绝对值非常小，导致![\sigma^{(L)}](https://math.jianshu.com/math?formula=%5Csigma%5E%7B(L)%7D)取值也非常小，使得基于梯度的**学习速度非常缓慢**。
    当使用交叉熵损失函数时，相对于输出层的导数(也可以被认为是残差)为![\sigma^{(L)}=a_k^{(L)}-y.](https://math.jianshu.com/math?formula=%5Csigma%5E%7B(L)%7D%3Da_k%5E%7B(L)%7D-y.)此导数是**线性**的，因此不会存在学习速度过慢的问题。

神经网络训练技巧

1. 解决过拟合的方法有：数据集增强、参数范数惩罚/正则化、模型集成等；其中Dropout是**模型集成方法**中最高效与常用的技巧。
    同时，深度神经网络的训练中涉及诸多手调参数，如学习率、权重衰减系数、Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。批量归一化(Batch Normalization，BN)方法有效规避了这些复杂参数对网络训练产生的影响，在**加速训练收敛**的同时也**提升了网络的泛化能力**。
2. 神经网络训练时是否可以将全部参数初始化为0？
    不能。
    考虑全连接的深度神经网络，同一层的神经元都是**同构**的，拥有相同的输入和输出，如果再将参数全部初始化为相同的值，那么无论前向传播还是反向传播的取值都是相同的。学习过程将永远无法打破这种**对称性**，最终同一个网络层中的各个参数仍然是相同的。
    因此需要随机地初始化神经网络参数的值，以打破对称性，简单来说，我们可以初始化参数为![(-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}})](https://math.jianshu.com/math?formula=(-%5Cfrac%7B1%7D%7B%5Csqrt%7Bd%7D%7D%2C%5Cfrac%7B1%7D%7B%5Csqrt%7Bd%7D%7D))的均匀分布，其中![d](https://math.jianshu.com/math?formula=d)为神经元接受的输入维度，偏置可以被简单地设为0，并不会导致参数对称的问题。
3. Dropout抑制过拟合的工作原理：Dropout随机丢弃部分神经元的机制，相当于每次迭代都在**训练不同结构的神经网络**。类比于Bagging方法，Dropout可以被认为是一种实用的**大规模深度神经网络的模型集成算法**。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在**小批量级别**上的操作，提供了一种**轻量级的Bagging集成**近似，能够实现指数级数量神经网络的训练与评测。
    对于包含![N](https://math.jianshu.com/math?formula=N)个神经元节点的网络，在Dropout的作用下可以看作![2^N](https://math.jianshu.com/math?formula=2%5EN)个模型的集成。这![2^N](https://math.jianshu.com/math?formula=2%5EN)个模型可认为是原始网络的子网络，它们共享部分权重，并且具有相同的网络层数，而模型整体的参数数目不变，大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程为**减弱全体神经元之间的联合适应性**，减少过拟合的风险，增强泛化能力。
4. 批量归一化的基本动机与原理：神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将会大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。然而随着网络训练的进行，每个隐层的参数变化使得**后一层的输入发生变化**，从而每一批训练数据的分布也随之变化，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。
    批量归一化方法是针对每一批数据，**在网络每一层输入之间增加归一化处理**(均值为0，标准差为1)，将所有批数据强制性在同一的数据分布下。可以看作在每一层输入和上一层输出之间**加入了一个新的计算层**，对数据的分布进行了额外的约束，从而增强模型的泛化能力。
    但是批量归一化同时也**降低了模型的拟合能力**，归一化之后的输入分被强制为0均值和1标准差，以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的**非饱和区域**，只包含线性变换，破坏了之前学习到的特征分布，为了恢复原始数据分布，具体实现中加入了变换重构以及可学习参数![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)和![\beta](https://math.jianshu.com/math?formula=%5Cbeta)：![y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)},](https://math.jianshu.com/math?formula=y%5E%7B(k)%7D%3D%5Cgamma%5E%7B(k)%7D%5Chat%7Bx%7D%5E%7B(k)%7D%2B%5Cbeta%5E%7B(k)%7D%2C)其中![\gamma^{(k)}](https://math.jianshu.com/math?formula=%5Cgamma%5E%7B(k)%7D)和![\beta^{(k)}](https://math.jianshu.com/math?formula=%5Cbeta%5E%7B(k)%7D)分别为输入数据分布的方差和偏差。
    对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重(对应复杂的非线性)。而在批量归一化操作中，![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)和![\beta](https://math.jianshu.com/math?formula=%5Cbeta)变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的过程，提高模型的泛化能力。

### 4.2 非线性模拟数据回归案例分析

- Pytorch下载
  https://pytorch.org/get-started/locally/ 选择对应的版本命令
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/2c205492823249b981865a3ec73368b3.png)
  
  ![](./imgs/logo.png)
  
- 加载数据

```python
import torch
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y < 50.0]
y = y[y < 50.0]
X_train, X_test, y_train, y_test = train_test_split(X, y)
standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train = standardScaler.transform(X_train)
X_test = standardScaler.transform(X_test)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```

((367, 13), (123, 13), (367,), (123,))

- 训练

  ![](./imgs/logo.png)

```python
#net
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, 100)
        self.predict = torch.nn.Linear(100, n_output)
    def forward(self, x):
        out = self.hidden(x)
        out = torch.relu(out)
        out = self.predict(out)
        return out
```

```python
net = Net(13, 1)
#loss
loss_func = torch.nn.MSELoss()
#optimiter
optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
#training
for i in range(10000):
    x_data = torch.tensor(X_train, dtype=torch.float32)
    y_data = torch.tensor(y_train, dtype=torch.float32)
    pred = net.forward(x_data)
   #   squeeze(a)就是将a中所有为1的维度删掉
    pred = torch.squeeze(pred)
    loss = loss_func(pred, y_data) * 0.001

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print("ite:{}, loss_train:{}".format(i, loss))
    print(pred[0:10])
    print(y_data[0:10])

    #test
    x_data = torch.tensor(X_test, dtype=torch.float32)
    y_data = torch.tensor(y_test, dtype=torch.float32)
    pred = net.forward(x_data)
    pred = torch.squeeze(pred)
    loss_test = loss_func(pred, y_data) * 0.001
    print("ite:{}, loss_test:{}".format(i, loss_test))

torch.save(net, "boston_model.pkl")
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/9295f59272fe4268b401872033a5eb2f.png)

- 加载模型测试

```python
net = torch.load("boston_model.pkl")
loss_func = torch.nn.MSELoss()
#test
x_data = torch.tensor(X_test, dtype=torch.float32)
y_data = torch.tensor(y_test, dtype=torch.float32)
pred = net.forward(x_data)
pred = torch.squeeze(pred)
loss_test = loss_func(pred, y_data) * 0.001
print("loss_test:{}".format(loss_test))
```

loss_test:0.015603514388203621

### 4.3 汽车燃料效率建模

![](./imgs/logo.png)

```
# 导包
import pathlib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
```

import pathlib # 引入文件路径操作的模块
import pandas as pd # 引入panda科学数据处理包
import seaborn as sns # 引入seaborn——绘图包，在pycharm中使用，需要调用pyplot的show()
import matplotlib.pyplot as plt # 标准绘图包
import tensorflow as tf # 引入tensorflow包
from tensorflow import keras # 引用tf包下的keras——这就是我们将用使用的高级API
from tensorflow.keras import layers # 引出layers模块，这是网络层相关的模块

```
dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
dataset_path  # 可查看数据——jupyter notebook可直接显示，pycharm需要print
```

- 设置数据标签

  由于读取的数据是是一个data文件，本身就类似于表格，所以我们需要为其读取前，设置好对应的列名-标签。

  ```
  column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',
                  'Acceleration', 'Model Year', 'Origin']
  
  ```

  **数据标签含义：**

  - MPG：每加仑燃料所行英里数
  - Cylinders：气缸数
  - Displacement：排量
  - Horsepower：马力
  - Weight：重量
  - Acceleration：加速能力
  - Model Year：车型年份——即生产（研发）年份
  - Origin：产源地

  使用panda读取数据集

  需要说明以下主要参数：依次为路径,索引名，空(无效)值设置，指定注释行，分隔符，是否考虑为False的数据

  ```
  raw_dataset = pd.read_csv(dataset_path, names=column_names,
                        na_values = "?", comment='\t',
                        sep=" ", skipinitialspace=True)
  ```

  参数含义对应展示：

  dataset_path——读取路径
  names——数据读取后的列索引名
  na_values——空(无效)值设置，这里设置为？
  comment——指定‘\t’对应的行为注释行，不读取
  sep——数据分隔符
  skipinitialspace——是否考虑为False的数据，即数据中存在False的bool值是被允许的

  ```
  raw_dataset = pd.read_csv(dataset_path, names=column_names,
                        na_values = "?", comment='\t',
                        sep=" ", skipinitialspace=True)
  
  dataset = raw_dataset.copy()  # 复制读取的数据，避免修改元数据
  dataset.tail()  # 显示表格数据——jupyter自显，pycharm需要print
  ```

对数据进行处理（一）
主要是删除无效值和数据分类操作。

1. 数据清洗——首先是无效数据查看：isna()会返回为包含无效值的表单，再追加一个sum()实现计算无效值的和；其次，对无效数据进行处理：dropna()清除无效数据后返回一个新数据表单。
2. 将需要的产源地分类序号从表中提取出来，使用pop(‘Origin’)提取Origin列的数据
3. 将Origin进行one-hot转换——即Origin不同的值仅对应一个数据有效：
4. dataset['USA'] = (origin == 1)*1.0*
5. *dataset['Europe'] = (origin == 2)*1.0
6. dataset['Japan'] = (origin == 3)*1.0——如此，仅当对应的标号时，某一列中的数据才为1，满足one-hot定义：[0,0,1,0,0……1，0],只有有效数据为1，其余为零。

```
dataset.isna().sum()  # 显示无效数据分布情况
```

```
dataset = dataset.dropna()  # 移除无效数据
```

```
origin = dataset.pop('Origin')  # 获取分类序号——以供one-hot转换需要
```

```
dataset['USA'] = (origin == 1)*1.0  
# one-hot转换：只有当列数据中值为1时，才设为1，否则为0，也就相当于一整列数据按 (origin == 1)条件转换为1 or 0

dataset['Europe'] = (origin == 2)*1.0
dataset['Japan'] = (origin == 3)*1.0
dataset.tail()  # 显示表格数据
```

对数据进行处理（二）
主要是拆分数据集和获取数据状况分析信息。

1. 采用sample(frac=0.8,random_state=0)——随机拆分80%的panda的DataFrame数据作为训练集
2. 采用drop(train_dataset.index)——移除已经取出的数据下标包含的数据并返回。
3. describe()用于观察一系列数据的范围，大小、波动趋势等等。（会存到train_stats）
4. 补充说明，描述信息需要pop(‘MPG’)——因为这是我们需要单独处理的数据。

```
train_dataset = dataset.sample(frac=0.8,random_state=0)  # 拆分获得训练数据
test_dataset = dataset.drop(train_dataset.index)  # 拆分获得测试数据
```

```
train_stats = train_dataset.describe()  # 获取数据的一系列描述信息
train_stats.pop("MPG")  # 移除MPG的数据列
train_stats = train_stats.transpose()  # 行列转换——也就是相当于翻转
train_stats  # 显示描述信息——jupyter自显
```

对数据进行处理（三）

主要是**分离训练数据的标签**——从获得的数据中。

1. 同样采用pop(‘MPG’)——获取指定列的数据

   ```
   train_labels = train_dataset.pop('MPG')   # 将移除的MPG列数据返回，赋值给train_labels 
   test_labels = test_dataset.pop('MPG')
   train_labels   # 显示数据
   test_labels 
   ```

   数据归一化

   在这次的数据中，虽然说只是一些简单数据的分析，但是数据归一化，特征归一化还是很有必要的。这样有助于数据收敛，得到更好的训练结果。
   采用的计算方式如下：x为需要归一化的数据，train_stats为当前表格的描述信息——通过以下运算就实现了归一化。（0 **or** 1）

   ```
   (x - train_stats['mean']) / train_stats['std']
   ```

   （采用函数，方便多次调用，避免公式重复导致可读性，降低人为失误）

   ```
   def norm(x):
     return (x - train_stats['mean']) / train_stats['std']
   normed_train_data = norm(train_dataset)  # 获取训练数据中的归一化数据
   normed_test_data = norm(test_dataset)  # 获取测试归一化数据
   ```

2. **本模型采用线性模型**——包含两个紧密相连的隐藏层，以及返回单个、连续值得输出层。
   为了代码结构，采用函数式编程：执行模型——网络层结构创建，以及模型编译。

   ```
   # 第一层Dense的input_shape为输入层大小，前边的64为该全连接层的输出层（隐藏层）——只有最后的全连接层 layers.Dense(1)输出才是输出层，否则为隐藏层。
   # activation为激活函数——这里是线性激活
   def build_model():
       model = keras.Sequential([
       layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),
       layers.Dense(64, activation='relu'),
       layers.Dense(1)
       ])
   
       optimizer = tf.keras.optimizers.RMSprop(0.001)
   
       model.compile(loss='mse',
       optimizer=optimizer,
       metrics=['mae', 'mse'])
       return model
   ```

   ```
   model = build_model()  # 创建一个模型
   model.summary()  # 展示模型结构
   ```

3. 采用上边创建好的模型进行预测——查看效果。

   ```
   example_batch = normed_train_data[:10]  # 获取十个数据来预测
   example_result = model.predict(example_batch)
   example_result
   ```

4. 数据拟合： 在这里添加一个输出点的函数——目的是显示训练的进程（提示：本次训练拟合周期（次数）为1000，你也可以进行更多的尝试）

   ```
   # 通过为每个完成的时期打印一个点来显示训练进度
   class PrintDot(keras.callbacks.Callback):
       def on_epoch_end(self, epoch, logs):
           if epoch % 100 == 0: print('')
           print('.', end='')
   EPOCHS = 1000  # 拟合次数
   
   # 返回的history为一个对象，内包含loss信息等
   history = model.fit(
     train_data, train_labels,
     epochs=EPOCHS, validation_split = 0.2, verbose=0,
     callbacks=[PrintDot()])    
   ```

   拟合fit函数的参数依次为：

   训练所需的归一化数据，训练数据对应的标签；
   拟合次数
   validation_split用于在没有提供验正集的时候，按一定比例从训练集中取出一部分作为验证集
   日志显示配置：verbose = 0 为不在标准输出流输出日志信息
   verbose = 1 为输出进度条记录
   verbose = 2 为每个epoch输出一行记录
   注意： 默认为 1
   callbacks——简单的说就是提供训练中的操作

5. 模型日志的查看

   ```
   hist = pd.DataFrame(history.history)  # 返回一个DataFrame对象，包含history的history数据
   hist['epoch'] = history.epoch  # 末尾添加一列数据包含epoch信息
   hist.tail()  # 展示表单数据
   ```

   

   

### 4.4 葡萄酒分类

![](./imgs/wine.png)

![](./imgs/logo.png)

```
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

import pandas as pd

# 导入数据
wine = load_wine()
# 划分训练集与测试集
x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, stratify=wine.target)
# 显示数据
frame = pd.DataFrame(x_train)
frame.columns = wine.feature_names
print(frame.head())

# 多层感知机,设置隐含层为1层100
model = MLPClassifier(hidden_layer_sizes=(100,))
model.fit(x_train, y_train)
predict_y = model.predict(x_test)
# 测试精度
score = accuracy_score(y_test, predict_y)
print(f"第一次测试的精度为:{score}")

# 使用标准预处理模块预处理数据,将原始数据归一化
scaler = StandardScaler()
fit = scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
# 再次训练并预测
model.fit(x_train, y_train)
predict_y = model.predict(x_test)
score = accuracy_score(y_test, predict_y)
print(f"第二次测试的精度为:{score}")

```





# 第5阶段--卷积神经网络

### 5.1卷积神经网络基础

线性神经网络局限性

任意多个隐层的神经网络和单层的神经网络都没有区别，而且都是线性的，而且线性模型的能够解决的问题也是有限的

神经网络的种类

- 基础神经网络：线性神经网络，BP神经网络，Hopfield神经网络等﻿
- 进阶神经网络：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等﻿
- 深度神经网络：深度置信网络，卷积神经网络，循环神经网络，LSTM网络等

卷积神经网络

![在这里插入图片描述](https://img-blog.csdnimg.cn/e6543a61af6a4501a5f7dfa1073e27f1.png)

传统意义上的多层神经网络是只有输入层、隐藏层、输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适卷积神经网络CNN，在原来多层神经网络的基础上，加入了更加有效的特征学习部分，具体操作就是在原来的全连接的层前面加入了部分连接的卷积层与池化层。卷积神经网络出现，使得神经网络层数得以加深，深度学习才能实现, 通常所说的深度学习，一般指的是这些CNN等新的结构以及一些新的方法（比如新的激活函数Relu等），解决了传统多层神经网络的一些难以解决的问题
![在这里插入图片描述](https://img-blog.csdnimg.cn/6291698fabcd495e823bb12b4a4e3d8c.png)卷积神经网络三个结构

神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层)以及激活层。每一层的作用

- 卷积层：通过在原始图像上平移来提取特征
- 激活层：增加非线性分割能力
- 池化层：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）

为了能够达到分类效果，还会有一个全连接层(FC)也就是最后的输出层，进行损失计算分类。
卷积层

卷积层（Convolutional layer）

![在这里插入图片描述](https://img-blog.csdnimg.cn/f78fabee782349d89597ece48bc2ee65.png)

卷积神经网络中每层卷积层由若干**卷积单元(卷积核)**组成，每个卷积单元的参数都是通过反向传播算法最佳化得到的。

**卷积运算的目的是提取输入的不同特征**，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网路能从低级特征中迭代提取更复杂的特征。

卷积核(Filter)的四大要素

- 卷积核个数
- 卷积核大小
- 卷积核步长
- 卷积核零填充大小

接下来我们通过计算案例讲解，假设图片是黑白图片（只有一个通道），一张像素值表

卷积如何计算-大小

卷积核我们可以理解为一个观察的人，带着若干权重和一个偏置去观察，进行特征加权运算。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ba3eee15b98b434483a57ca7a0c1f63d.png)
    ：上述要加上偏置

    卷积核大小
        1*1、3*3、5*5

通常卷积核大小选择这些大小，是经过研究人员证明比较好的效果。这个人观察之后会得到一个运算结果，

那么这个人想观察所有这张图的像素怎么办？那就需要这样
![在这里插入图片描述](https://img-blog.csdnimg.cn/f122f66f753d467bb7470540de2bbc67.gif)

卷积如何计算-步长

需要去移动卷积核观察这张图片，需要的参数就是步长。
假设移动的步长为一个像素，那么最终这个人观察的结果以下图为例：
5x5的图片，3x3的卷积大小去一个步长运算得到3x3的大小观察结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/c2f1e8a20b3d4a28918511c57f560fb1.png)
如果移动的步长为2那么结果是这样

5x5的图片，3x3的卷积大小去两个步长运算得到2x2的大小观察结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/6e53723ceb01426e9def019b63933079.png)
卷积如何计算-卷积核个数

那么如果在某一层结构当中，不止是一个人观察，多个人(卷积核)一起去观察。那就得到多张观察结果。

不同的卷积核带的权重和偏置都不一样，即随机初始化的参数

我们已经得出输出结果的大小有大小和步长决定的，但是只有这些吗，还有一个就是零填充。Filter观察窗口的大小和移动步长会导致超过图片像素宽度！

卷积如何计算-零填充大小

零填充就是在图片像素外围填充一圈值为0的像素。

![在这里插入图片描述](https://img-blog.csdnimg.cn/fe4494f82b8840368820dc80aa7516fc.png)
有两种方式，SAME和VALID

    SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致。
    VALID：不越过边缘取样，取样的面积小于输入人的图像的像素宽度。
输出大小计算公式
最终零填充到底填充多少呢？我们并不需要去关注，接下来我们利用已知的这些条件来去求出输出的大小来看结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/247d6b127d0a48e389328f910392f64c.png)
通过一个例子来理解下面的公式

    计算案例：
    
    1、假设已知的条件：输入图像32*32*1, 50个Filter，大小为5*5，移动步长为1，零填充大小为1。请求出输出大小？
    
    H2 = (H1 - F + 2P)/S + 1 = (32 - 5 + 2 * 1)/1 + 1 = 30
    
    W2 = (H1 - F + 2P)/S + 1 = (32 -5 + 2 * 1)/1 + 1 = 30
    
    D2 = K = 50
    
    所以输出大小为[30, 30, 50]
    
    2、假设已知的条件：输入图像32*32*1, 50个Filter，大小为3*3，移动步长为1，未知零填充。输出大小32*32？
    
    H2 = (H1 - F + 2P)/S + 1 = (32 - 3 + 2 * P)/1 + 1 = 32
    
    W2 = (H1 - F + 2P)/S + 1 = (32 -3 + 2 * P)/1 + 1 = 32
    
    所以零填充大小为：1*1

多通道图片如何观察

如果是一张彩色图片，那么就有三种表分别为R，G，B。原本每个人需要带一个3x3或者其他大小的卷积核，现在需要带3张3x3的权重和一个偏置，总共就27个权重。最终每个人还是得出一张结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/bdcde4f56b8849d6b990a2014fc1f9e9.png)

激活函数

卷积网络结构采用激活函数，自从网路得到发展之后。大家发现原有的sigmoid这些激活函数并不能达到好的效果，所以采取新的激活函数。
-  Relu
-![在这里插入图片描述](https://img-blog.csdnimg.cn/c4a6cac30e8f4856bfc7543fa94096c7.png)
效果是什么样的呢？
![在这里插入图片描述](https://img-blog.csdnimg.cn/925266a079e94bf398b99ef84a2eb6fc.png)

- Relu优点
有效解决梯度爆炸问题
计算速度非常快，只需要判断输入是否大于0。SGD(批梯度下降)的求解速度速度远快于sigmoid和tanh
- sigmoid缺点
采用sigmoid等函数，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。在深层网络中，sigmoid函数 反向传播 时，很容易就会出现梯度梯度爆炸的情况

池化层(Polling)

Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，通常采用最大池化

    max_polling:取池化窗口的最大值
    avg_polling:取池化窗口的平均值
![在这里插入图片描述](https://img-blog.csdnimg.cn/3cc45d8ed21f46ad9a4d9d9fe4794426.png)
池化层计算

池化层也有窗口的大小以及移动步长，那么之后的输出大小怎么计算？计算公式同卷积计算公式一样

    计算：224x224x64,窗口为2，步长为2输出结果？
    
    H2 = (224 - 2 + 2*0)/2 +1 = 112
    
    w2 = (224 - 2 + 2*0)/2 +1 = 112

通常池化层采用 2x2大小、步长为2窗口

BN层

目的：提高网络泛化能力，防止过拟合

BN(Batch Normalization)也属于网络的一层，又称为归一化层。使用BN的好处，包括可以使用更大的学习率,成为CNN的标配

![在这里插入图片描述](https://img-blog.csdnimg.cn/e53c646734164efb8e2f569ee2b9d03f.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/3b2efde3d0864567b779aa35fc7b53fd.png)


Full Connection层

前面的卷积和池化相当于做特征工程，最后的全连接层在整个卷积神经网络中起到“分类器”的作用。 
![在这里插入图片描述](https://img-blog.csdnimg.cn/2de5489314ae4e2d8400a5c7d5b04a7f.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/275167bb2fde419cbd87bc154d45434c.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/6b1658f0ba6f4fb78e4cb2e4d6ea79c3.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/29720f6c6b02411e9ba2e20e3af1d459.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/ff49882823d94d3c997ab0a36808a071.png)


梯度下降不同优化版本
![在这里插入图片描述](https://img-blog.csdnimg.cn/d9a00d9b7ed94a06877b00cd56ae1fba.gif)

最朴素的优化算法就是SGD了，梯度下降算法效果也很好，但也存在一些问题选择一个合理的学习速率很难。容易陷入那些次优的局部极值点中
```
拓展内容（了解）：

    SGD with Momentum

梯度更新规则:Momentum在梯度下降的过程中加入了惯性，使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。

    RMSProp

梯度更新规则:解决Adagrad学习率急剧下降的问题，RMSProp改变了二阶动量计算方法，即用窗口滑动加权平均值计算二阶动量。

    Adam

梯度更新规则:Adam = Adaptive + Momentum，顾名思义Adam集成了SGD的一阶动量和RMSProp的二阶动量
```
卷积神经网络发展历史

![在这里插入图片描述](https://img-blog.csdnimg.cn/7c8575cd1a0644098a8cd24c32b4a90b.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/0238e2d0cec9406994854cecbb4dd685.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/41711577307c4199b7af4dd7e3ce23a9.jpeg)

![在这里插入图片描述](https://img-blog.csdnimg.cn/659e348e5ff843b09d798004f54f2d1d.png)

卷积网络其它用途

    图像目标检测
        Yolo：GoogleNet+ bounding boxes
        SSD：VGG + region proposals


![在这里插入图片描述](https://img-blog.csdnimg.cn/d334ec71374743ed91d3528bdf904c48.png)

简单CNN的搭建

![](./imgs/logo.png)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

```


```python
class Net(nn.Module):
    def __init__(self): #定义神经网络结构, 输入数据 1x32x32
        super(Net, self).__init__()
        # 第一层（卷积层）
        self.conv1 = nn.Conv2d(1,6,3) #输入频道1， 输出频道6， 卷积3x3
        # 第二层（卷积层）
        self.conv2 = nn.Conv2d(6,16,3) #输入频道6， 输出频道16， 卷积3x3
        # 第三层（全连接层）
        self.fc1 = nn.Linear(16*28*28, 512) #输入维度16x28x28=12544，输出维度 512
        # 第四层（全连接层）
        self.fc2 = nn.Linear(512, 64) #输入维度512， 输出维度64
        # 第五层（全连接层）
        self.fc3 = nn.Linear(64, 2) #输入维度64， 输出维度2
    
    def forward(self, x): #定义数据流向
        x = self.conv1(x)
        x = F.relu(x)
        
        x = self.conv2(x)
        x = F.relu(x)
        
        x = x.view(-1, 16*28*28)
        x = self.fc1(x)
        x = F.relu(x)
        
        x = self.fc2(x)
        x = F.relu(x)
        
        x = self.fc3(x)
        
        return x
        
```


```python
net = Net()
print(net)
```

    Net(
      (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
      (fc1): Linear(in_features=12544, out_features=512, bias=True)
      (fc2): Linear(in_features=512, out_features=64, bias=True)
      (fc3): Linear(in_features=64, out_features=2, bias=True)
    )



```python
#生成随机输入
input_data = torch.randn(1,1,32,32) 
print(input_data)
print(input_data.size())
```

    tensor([[[[ 0.3055, -0.8828,  0.1044,  ..., -0.4833,  1.1879, -0.0727],
              [ 0.2718, -1.5784, -1.0362,  ..., -0.5160,  0.4685, -0.5401],
              [ 2.4876,  0.1718,  1.2377,  ..., -0.6047, -0.7236,  0.3888],
              ...,
              [-0.8249, -0.3313, -0.3513,  ...,  0.2470, -0.6509, -0.9969],
              [ 1.0528,  0.0348,  0.6416,  ..., -0.4129, -0.1997,  0.1648],
              [ 1.5184,  0.0120, -2.3959,  ..., -1.3124, -0.4289, -0.2882]]]])
    torch.Size([1, 1, 32, 32])



```python
# 运行神经网络
out = net(input_data)
print(out)
print(out.size())
```

    tensor([[-0.0375, -0.0235]], grad_fn=<AddmmBackward>)
    torch.Size([1, 2])



```python
# 随机生成真实值
target = torch.randn(2)
target = target.view(1,-1)
print(target)
```

    tensor([[-2.1838, -0.4858]])



```python
criterion = nn.L1Loss() # 定义损失函数
loss = criterion(out, target) # 计算损失
print(loss)
```

    tensor(1.3043, grad_fn=<L1LossBackward>)



```python
# 反向传递
net.zero_grad() #清零梯度
loss.backward() #自动计算梯度、反向传递

```


```python
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.01)
optimizer.step()
```


```python
out = net(input_data)
print(out)
print(out.size())
```

    tensor([[-0.0946, -0.0601]], grad_fn=<AddmmBackward>)
    torch.Size([1, 2])


- 第二次是权值更新损失要比第一次要小一些


```python
criterion = nn.L1Loss() # 定义损失函数  MAE
loss = criterion(out, target) # 计算损失
print(loss)
```

    tensor(1.2574, grad_fn=<L1LossBackward>)

### 5.2 使用CNN 进行Mnist手写数字分类

![](./imgs/logo.png)

- 导包个加载mnist数据集
```python
import torch
import torchvision.datasets as dataset
import torchvision.transforms as transforms
import torch.utils.data as data_utils
#data
train_data = dataset.MNIST(root="mnist",
                           train=True,
                           transform=transforms.ToTensor(),
                           download=True)

test_data = dataset.MNIST(root="mnist",
                           train=False,
                           transform=transforms.ToTensor(),
                           download=False)
#batchsize
train_loader = data_utils.DataLoader(dataset=train_data,
                                     batch_size=64,
                                     shuffle=True)

test_loader = data_utils.DataLoader(dataset=test_data,
                                     batch_size=64,
                                     shuffle=True)

# 随机加载一个batch进行可视化
def show():
    def imageshow(img, labels):
        # 接受的是一个<class 'torch.Tensor'>
        # c h w
        img = img / 2 + 0.5  # 变换灰度
        nimg = img.numpy()
        nimg = np.transpose(nimg, (1, 2, 0))  # 变成 [h, w, c]
        plt.imshow(nimg)

        plt.xlabel(labels)
        plt.show()

    # 加载一个mini batch
    for data in train_loader:
        images, labels = data
        break
    print(images.size())
    print(type(images))
    print(type(labels))
    print(type(labels))
    imageshow(torchvision.utils.make_grid(images), labels)
show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/346b9a74ffdc40dca5e04f169529b1af.png)

- 构建CNN
```python
class CNN(torch.nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv =torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, kernel_size=5, padding=2), ## (28-5 + 4)/1 + 1 = 28
            torch.nn.BatchNorm2d(32),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(2) # 28--> 14
        )
        self.fc = torch.nn.Linear(14 * 14 * 32, 10)
    def forward(self, x):
        out = self.conv(x)
        out = out.view(out.size()[0], -1)
        out = self.fc(out)
        return out
```
- 加载模型测试
```python
class CNN(torch.nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv =torch.nn.Sequential(
            torch.nn.Conv2d(1, 32, kernel_size=5, padding=2),
            torch.nn.BatchNorm2d(32),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(2)
        )
        self.fc = torch.nn.Linear(14 * 14 * 32, 10)
    def forward(self, x):
        out = self.conv(x)
        out = out.view(out.size()[0], -1)
        out = self.fc(out)
        return out
cnn = CNN()
# cnn = cnn.cuda()
#loss

loss_func = torch.nn.CrossEntropyLoss()

#optimizer

optimizer = torch.optim.Adam(cnn.parameters(), lr=0.01)

#training
for epoch in range(10):
    for i, (images, labels) in enumerate(train_loader):
#         images = images.cuda()
#         labels = labels.cuda()
        outputs = cnn(images)
        loss = loss_func(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

      print("epoch is {}, ite is "
            "{}/{}, loss is {}".format(epoch+1, i,
                                       len(train_data) // 64, # 总量//64
                                       loss.item()))
    #eval/test
    loss_test = 0
    accuracy = 0
    for i, (images, labels) in enumerate(test_loader):
#         images = images.cuda()
#         labels = labels.cuda()
        outputs = cnn(images)
        #[batchsize]
        #outputs = batchsize * cls_num
        loss_test += loss_func(outputs, labels)
        _, pred = outputs.max(1)
        accuracy += (pred == labels).sum().item()

    accuracy = accuracy / len(test_data)
    loss_test = loss_test / (len(test_data) // 64)

    print("epoch is {}, accuracy is {}, "
          "loss test is {}".format(epoch + 1,
                                   accuracy,
                                   loss_test.item()))

torch.save(cnn, "mnist_model.pkl")
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/91b4d2a15ff64b369daa5b5d8f6a09fe.png)
```python

cnn = torch.load("mnist_model.pkl")
# cnn = cnn.cuda()
#loss
#eval/test
loss_test = 0
accuracy = 0

import cv2
#pip install opencv-python -i http://mirrors.aliyun.com/pypi/simple/   --trusted-host mirrors.aliyun.com
for i, (images, labels) in enumerate(test_loader):
#     images = images.cuda()
#     labels = labels.cuda()
    outputs = cnn(images)
    _, pred = outputs.max(1)
    accuracy += (pred == labels).sum().item()

    images = images.cpu().numpy()
    labels = labels.cpu().numpy()
    pred = pred.cpu().numpy()
    #batchsize * 1 * 28 * 28

    for idx in range(images.shape[0]):
        im_data = images[idx]
        im_label = labels[idx]
        im_pred = pred[idx]
        im_data = im_data.transpose(1, 2, 0)
accuracy = accuracy / len(test_data)
print(accuracy)
```
0.9824

### 5.3 ciffar10 数据集图像分类

CNN构建Cifar10图像分类器

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210714085958818.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210714090015106.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210714090102531.png)

![](./imgs/logo.png)

```python
import torch
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
```


```python
# (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)  第一个是rgb的均值, 第二个是rgb三通道的方差  都指定为0.5
# 标准化 归一化
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ## 均值(三通道) 方差(三通道)
    ]
    )

#训练数据集
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, 
                                       download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,
                                         shuffle=True, num_workers=2)

#测试数据集
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                      download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=16,
                                        shuffle=False, num_workers=2)



```

    Files already downloaded and verified
    Files already downloaded and verified



```python
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

def imshow(img):
    # 输入数据: torch.tensor [c, h, w]
    img = img / 2+0.5
    nping = img.numpy()
    nping = np.transpose(nping, (1,2,0)) # [h,w,c]
    plt.imshow(nping)
    
dataiter = iter(trainloader) #随机加载一个mini batch
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/595c464649964fe6aeaeaeb564cf8ad6.png)


```python
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self): #定义神经网络结构, 输入数据 3x32x32
        super(Net, self).__init__()
        # 第一层（卷积层）
        self.conv1 = nn.Conv2d(3,6,3) #输入频道3， 输出频道6， 卷积3x3
        # 第二层（卷积层）
        self.conv2 = nn.Conv2d(6,16,3) #输入频道6， 输出频道16， 卷积3x3
        # 第三层（全连接层）
        self.fc1 = nn.Linear(16*28*28, 512) #输入维度16x28x28=12544，输出维度 512
        # 第四层（全连接层）
        self.fc2 = nn.Linear(512, 64) #输入维度512， 输出维度64
        # 第五层（全连接层）
        self.fc3 = nn.Linear(64, 10) #输入维度64， 输出维度10
    
    def forward(self, x): #定义数据流向
        x = self.conv1(x)
        x = F.relu(x)
        
        x = self.conv2(x)
        x = F.relu(x)
        
        x = x.view(-1, 16*28*28)
        x = self.fc1(x)
        x = F.relu(x)
        
        x = self.fc2(x)
        x = F.relu(x)
        
        x = self.fc3(x)
        
        return x
        
```


```python
net = Net()
print(net)
```

    Net(
      (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
      (fc1): Linear(in_features=12544, out_features=512, bias=True)
      (fc2): Linear(in_features=512, out_features=64, bias=True)
      (fc3): Linear(in_features=64, out_features=10, bias=True)
    )



```python
import torch.optim as optim

criterion = nn.CrossEntropyLoss()  # 交叉熵损失 # 
optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)
#  Momentum 梯度下降法，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行速度几乎总是快于标准的梯度下降算法。
```


```python
train_loss_hist = []
test_loss_hist = []
for epoch in range(2):
    for i, data in enumerate(trainloader):
        images, labels = data        
        outputs = net(images)
        loss = criterion(outputs, labels) # 计算损失
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if i%1000==0:
            print("Epoch: {} step: {} Loss: {}".format(epoch, i, loss.item()))


```

Epoch: 0 step: 0 Loss: 2.327638864517212
Epoch: 0 step: 1000 Loss: 2.2910702228546143
Epoch: 0 step: 2000 Loss: 2.303840160369873
Epoch: 0 step: 3000 Loss: 2.252164363861084
Epoch: 1 step: 0 Loss: 2.2408382892608643
Epoch: 1 step: 1000 Loss: 2.0526092052459717
Epoch: 1 step: 2000 Loss: 2.0468878746032715
Epoch: 1 step: 3000 Loss: 2.1996114253997803
```python
train_loss_hist = []
test_loss_hist = []

for epoch in tqdm(range(20)):
    #训练
    net.train()
    running_loss = 0.0
    for i, data in enumerate(trainloader):
        images, labels = data        
        outputs = net(images)
        loss = criterion(outputs, labels) # 计算损失
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        if(i%250 == 0): #每250 mini batch 测试一次
            correct = 0.0
            total = 0.0

            net.eval()
            with torch.no_grad():
                for test_data in testloader:
                    test_images, test_labels = test_data
                    test_outputs = net(test_images)
                    test_loss = criterion(test_outputs, test_labels)

            train_loss_hist.append(running_loss/250)
            test_loss_hist.append(test_loss.item())
            running_loss=0.0


```



    100%|██████████| 20/20 [50:48<00:00, 148.08s/it]



```python
plt.figure()
plt.plot(temp)
plt.plot(test_loss_hist)
plt.legend(('train loss', 'test loss'))
plt.title('Train/Test Loss')
plt.xlabel('# mini batch *250')
plt.ylabel('Loss')
```

    Text(0,0.5,'Loss')


![](https://img-blog.csdnimg.cn/img_convert/468629f830a3d2c3cfdd693c8f3a9b9c.png)

# 第6阶段--循环神经网络

### 6.1 简介

RNN模型

RNN(Recurrent Neural Network), 中文称作循环神经网络, 它一般以序列数据为输入, 通过网络内部的结构设计有效捕捉序列之间的关系特征, 一般也是以序列形式进行输出.
-  RNN单层网络结构:


![请添加图片描述](https://img-blog.csdnimg.cn/ec86142f134446a1896a4a6b623d03d1.gif)
以时间步对RNN进行展开后的单层网络结构:

![请添加图片描述](https://img-blog.csdnimg.cn/564ce8e71bc94abb9d54458748e56527.gif)


### 6.2 历史

RNN的循环机制使模型隐层上一时间步产生的结果, 能够作为当下时间步输入的一部分(当下时间步的输入除了正常的输入外还包括上一步的隐层输出)对当下时间步的输出产生影响RNN模型的作用:因为RNN结构能够很好利用序列之间的关系, 因此针对自然界具有连续性的输入序列, 如人类的语言, 语音等进行很好的处理, 广泛应用于NLP领域的各项任务, 如文本分类, 情感分析, 意图识别, 机器翻译等
下面我们将以一个用户意图识别的例子进行简单的分析:![请添加图片描述](https://img-blog.csdnimg.cn/116d79d44f5540198308ad8690c8b7e6.gif)
第一步: 用户输入了"What time is it ?", 我们首先需要对它进行基本的分词, 因为RNN是按照顺序工作的, 每次只接收一个单词进行处理.

![请添加图片描述](https://img-blog.csdnimg.cn/e3a0c3bfba664e7eb9f1d5630f8be9c5.gif)

第二步: 首先将单词"What"输送给RNN, 它将产生一个输出O1.

![请添加图片描述](https://img-blog.csdnimg.cn/c6fc808afc1545038785091c7ad0becb.gif)

第三步: 继续将单词"time"输送给RNN, 但此时RNN不仅仅利用"time"来产生输出O2, 还会使用来自上一层隐层输出O1作为输入信息.![请添加图片描述](https://img-blog.csdnimg.cn/6a86e29c96ab42228cb2ba74e19dad93.gif)
第四步: 重复这样的步骤, 直到处理完所有的单词.
![请添加图片描述](https://img-blog.csdnimg.cn/66113a6a1cc54e7089117f81cfc090f4.gif)
第五步: 最后，将最终的隐层输出O5进行处理来解析用户意图.
![请添加图片描述](https://img-blog.csdnimg.cn/26e407518b4e451e89b1acf9e4842d0b.gif)
RNN模型的分类:

这里我们将从两个角度对RNN模型进行分类. 第一个角度是输入和输出的结构, 第二个角度是RNN的内部构造.
按照输入和输出的结构进行分类:

N vs N - RNN
N vs 1 - RNN
1 vs N - RNN
N vs M - RNN

按照RNN的内部构造进行分类:

传统RNN
LSTM
Bi-LSTM
GRU
Bi-GRU

- N vs N - RNN:
  它是RNN最基础的结构形式, 最大的特点就是: 输入和输出序列是等长的. 由于这个限制的存在, 使其适用范围比较小, 可用于生成等长度的合辙诗句


- ![请添加图片描述](https://img-blog.csdnimg.cn/94f4a06e81664e189ab8febcd379499c.png)
  N vs 1 - RNN:
  有时候我们要处理的问题输入是一个序列，而要求输出是一个单独的值而不是序列，应该怎样建模呢？我们只要在最后一个隐层输出h上进行线性变换就可以了，大部分情况下，为了更好的明确结果, 还要使用sigmoid或者softmax进行处理. 这种结构经常被应用在文本分类问题上
  ![请添加图片描述](https://img-blog.csdnimg.cn/4a320cdae96d456297c72fdf7f102b8f.png)
  1 vs N - RNN:
  如果输入不是序列而输出为序列的情况怎么处理呢？我们最常采用的一种方式就是使该输入作用于每次的输出之上. 这种结构可用于将图片生成文字任务等.
  ![请添加图片描述](https://img-blog.csdnimg.cn/fd49a83205b64b48a26d8094eaa6275d.png)

N vs M - RNN:
这是一种不限输入输出长度的RNN结构, 它由编码器和解码器两部分组成, 两者的内部结构都是某类RNN, 它也被称为seq2seq架构. 输入数据首先通过编码器, 最终输出一个隐含变量c, 之后最常用的做法是使用这个隐含变量c作用在解码器进行解码的每一步上, 以保证输入信息被有效利用.
![请添加图片描述](https://img-blog.csdnimg.cn/57dc649b8518439d9e86256217434bbe.png)

seq2seq架构最早被提出应用于机器翻译, 因为其输入输出不受限制，如今也是应用最广的RNN模型结构. 在机器翻译, 阅读理解, 文本摘要等众多领域都进行了非常多的应用实践.

传统RNN的内部结构图:
![在这里插入图片描述](https://img-blog.csdnimg.cn/e0aca5a8e47d4d309888513a341bc5e0.png)
结构解释图:

![在这里插入图片描述](https://img-blog.csdnimg.cn/905b56fcc0cc4226816fc4787373fc48.png)
内部结构分析:
我们把目光集中在中间的方块部分, 它的输入有两部分, 分别是h(t-1)以及x(t), 代表上一时间步的隐层输出, 以及此时间步的输入, 它们进入RNN结构体后, 会"融合"到一起, 这种融合我们根据结构解释可知, 是将二者进行拼接, 形成新的张量[x(t), h(t-1)], 之后这个新的张量将通过一个全连接层(线性层), 该层使用tanh作为激活函数, 最终得到该时间步的输出h(t), 它将作为下一个时间步的输入和x(t+1)一起进入结构体. 以此类推.

- 内部结构过程演示:
  ![请添加图片描述](https://img-blog.csdnimg.cn/a6e9f496f1964725bec7fd970e959511.gif)
  根据结构分析得出内部计算公式:
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/dd383dc8dbeb4fd39a918a252abcc590.png)

激活函数tanh的作用:
用于帮助调节流经网络的值, tanh函数将值压缩在-1和1之间.
![请添加图片描述](https://img-blog.csdnimg.cn/acbb5b3e4d824b8ebf891d965e92a89b.gif)
![在这里插入图片描述](https://img-blog.csdnimg.cn/7250183681d94c7bbeae790976658419.png)

```python
>>> import torch
>>> import torch.nn as nn
>>> rnn = nn.RNN(5, 6, 1)
>>> input = torch.randn(1, 3, 5)
>>> h0 = torch.randn(1, 3, 6)
>>> output, hn = rnn(input, h0)
>>> output
tensor([[[ 0.4282, -0.8475, -0.0685, -0.4601, -0.8357,  0.1252],
         [ 0.5758, -0.2823,  0.4822, -0.4485, -0.7362,  0.0084],
         [ 0.9224, -0.7479, -0.3682, -0.5662, -0.9637,  0.4938]]],
       grad_fn=<StackBackward>)

>>> hn
tensor([[[ 0.4282, -0.8475, -0.0685, -0.4601, -0.8357,  0.1252],
         [ 0.5758, -0.2823,  0.4822, -0.4485, -0.7362,  0.0084],
         [ 0.9224, -0.7479, -0.3682, -0.5662, -0.9637,  0.4938]]],
       grad_fn=<StackBackward>)
```

传统RNN的优势:
由于内部结构简单, 对计算资源要求低, 相比之后我们要学习的RNN变体:LSTM和GRU模型参数总量少了很多, 在短序列任务上性能和效果都表现优异.
传统RNN的缺点:
传统RNN在解决长序列之间的关联时, 通过实践，证明经典RNN表现很差, 原因是在进行反向传播的时候, 过长的序列导致梯度的计算异常, 发生梯度消失或爆炸.
什么是梯度消失或爆炸呢？
根据反向传播算法和链式法则, 梯度的计算可以简化为以下公式:
![avatar](https://img-blog.csdnimg.cn/e824687ace34419f94f3bbe7c0676013.png)

其中sigmoid的导数值域是固定的, 在[0, 0.25]之间, 而一旦公式中的w也小于1, 那么通过这样的公式连乘后, 最终的梯度就会变得非常非常小, 这种现象称作梯度消失. 反之, 如果我们人为的增大w的值, 使其大于1, 那么连乘够就可能造成梯度过大, 称作梯度爆炸.

LSTM模型

LSTM（Long Short-Term Memory）也称长短时记忆结构, 它是传统RNN的变体, 与经典RNN相比能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时LSTM的结构更复杂, 它的核心结构可以分为四个部分去解析:

- 遗忘门
- 输入门
- 细胞状态
- 输出门

LSTM的内部结构图:

![在这里插入图片描述](https://img-blog.csdnimg.cn/dc619f9bc381499892d51c668d001192.png)
结构解释图:
![在这里插入图片描述](https://img-blog.csdnimg.cn/90d655e1cae549d683fd7cdd7f936b88.png)
遗忘门部分结构图与计算公式:
![在这里插入图片描述](https://img-blog.csdnimg.cn/a3f45d6287fa438d8a19c86c77d5813f.png)
遗忘门结构分析:
与传统RNN的内部结构计算非常相似, 首先将当前时间步输入x(t)与上一个时间步隐含状态h(t-1)拼接, 得到[x(t), h(t-1)], 然后通过一个全连接层做变换, 最后通过sigmoid函数进行激活得到f(t), 我们可以将f(t)看作是门值, 好比一扇门开合的大小程度, 门值都将作用在通过该扇门的张量, 遗忘门门值将作用的上一层的细胞状态上, 代表遗忘过去的多少信息, 又因为遗忘门门值是由x(t), h(t-1)计算得来的, 因此整个公式意味着根据当前时间步输入和上一个时间步隐含状态h(t-1)来决定遗忘多少上一层的细胞状态所携带的过往信息.
遗忘门内部结构过程演示:
![请添加图片描述](https://img-blog.csdnimg.cn/87b66d65e8494cb2b69914346a127f3d.gif)

激活函数sigmiod的作用:
用于帮助调节流经网络的值, sigmoid函数将值压缩在0和1之间.![请添加图片描述](https://img-blog.csdnimg.cn/a4be557180294090a89e9e5c74b4678a.gif)
输入门部分结构图与计算公式:
![在这里插入图片描述](https://img-blog.csdnimg.cn/ef5b0d22a6fa484fa74322149af94dc2.png)
输入门结构分析:
我们看到输入门的计算公式有两个, 第一个就是产生输入门门值的公式, 它和遗忘门公式几乎相同, 区别只是在于它们之后要作用的目标上. 这个公式意味着输入信息有多少需要进行过滤. 输入门的第二个公式是与传统RNN的内部结构计算相同. 对于LSTM来讲, 它得到的是当前的细胞状态, 而不是像经典RNN一样得到的是隐含状态.
输入门内部结构过程演示:
![请添加图片描述](https://img-blog.csdnimg.cn/66d85fa8f07c47c8883a6b1a04da069b.gif)
细胞状态更新分析:
细胞更新的结构与计算公式非常容易理解, 这里没有全连接层, 只是将刚刚得到的遗忘门门值与上一个时间步得到的C(t-1)相乘, 再加上输入门门值与当前时间步得到的未更新C(t)相乘的结果. 最终得到更新后的C(t)作为下一个时间步输入的一部分. 整个细胞状态更新过程就是对遗忘门和输入门的应用.

细胞状态更新过程演示:
![请添加图片描述](https://img-blog.csdnimg.cn/76122485a3fa42b5b8ec5472f3e83acf.gif)
输出门部分结构图与计算公式:
![在这里插入图片描述](https://img-blog.csdnimg.cn/b1768b8619ed4e99963b28dd032640d0.png)
输出门结构分析:
输出门部分的公式也是两个, 第一个即是计算输出门的门值, 它和遗忘门，输入门计算方式相同. 第二个即是使用这个门值产生隐含状态h(t), 他将作用在更新后的细胞状态C(t)上, 并做tanh激活, 最终得到h(t)作为下一时间步输入的一部分. 整个输出门的过程, 就是为了产生隐含状态h(t).
输出门内部结构过程演示:
![请添加图片描述](https://img-blog.csdnimg.cn/781bbca6b14841babc253abf4c7f9a33.gif)
什么是Bi-LSTM ?
Bi-LSTM即双向LSTM, 它没有改变LSTM本身任何的内部结构, 只是将LSTM应用两次且方向不同, 再将两次得到的LSTM结果进行拼接作为最终输出.
![在这里插入图片描述](https://img-blog.csdnimg.cn/580fb395a0af4c15a4f3e063e26798e5.png)
Bi-LSTM结构分析:
我们看到图中对"我爱中国"这句话或者叫这个输入序列, 进行了从左到右和从右到左两次LSTM处理, 将得到的结果张量进行了拼接作为最终输出. 这种结构能够捕捉语言语法中一些特定的前置或后置特征, 增强语义关联,但是模型参数和计算复杂度也随之增加了一倍, 一般需要对语料和计算资源进行评估后决定是否使用该结构.

- Pytorch中LSTM工具的使用:
  位置: 在torch.nn工具包之中, 通过torch.nn.LSTM可调用.
- nn.LSTM类初始化主要参数解释:
  input_size: 输入张量x中特征维度的大小.
  hidden_size: 隐层张量h中特征维度的大小.
  num_layers: 隐含层的数量.
  nonlinearity: 激活函数的选择, 默认是tanh.
  bidirectional: 是否选择使用双向LSTM, 如果为True, 则使用; 默认不使用.

- nn.LSTM类实例化对象主要参数解释:
  input: 输入张量x.
  h0: 初始化的隐层张量h.
  c0: 初始化的细胞状态张量c.

```python
>>> import torch.nn as nn
>>> import torch
>>> rnn = nn.LSTM(5, 6, 2)
>>> input = torch.randn(1, 3, 5)
>>> h0 = torch.randn(2, 3, 6)
>>> c0 = torch.randn(2, 3, 6)
>>> output, (hn, cn) = rnn(input, (h0, c0))
>>> output
tensor([[[ 0.0447, -0.0335,  0.1454,  0.0438,  0.0865,  0.0416],
         [ 0.0105,  0.1923,  0.5507, -0.1742,  0.1569, -0.0548],
         [-0.1186,  0.1835, -0.0022, -0.1388, -0.0877, -0.4007]]],
       grad_fn=<StackBackward>)
>>> hn
tensor([[[ 0.4647, -0.2364,  0.0645, -0.3996, -0.0500, -0.0152],
         [ 0.3852,  0.0704,  0.2103, -0.2524,  0.0243,  0.0477],
         [ 0.2571,  0.0608,  0.2322,  0.1815, -0.0513, -0.0291]],

        [[ 0.0447, -0.0335,  0.1454,  0.0438,  0.0865,  0.0416],
         [ 0.0105,  0.1923,  0.5507, -0.1742,  0.1569, -0.0548],
         [-0.1186,  0.1835, -0.0022, -0.1388, -0.0877, -0.4007]]],
       grad_fn=<StackBackward>)
>>> cn
tensor([[[ 0.8083, -0.5500,  0.1009, -0.5806, -0.0668, -0.1161],
         [ 0.7438,  0.0957,  0.5509, -0.7725,  0.0824,  0.0626],
         [ 0.3131,  0.0920,  0.8359,  0.9187, -0.4826, -0.0717]],

        [[ 0.1240, -0.0526,  0.3035,  0.1099,  0.5915,  0.0828],
         [ 0.0203,  0.8367,  0.9832, -0.4454,  0.3917, -0.1983],
         [-0.2976,  0.7764, -0.0074, -0.1965, -0.1343, -0.6683]]],
       grad_fn=<StackBackward>)
```

LSTM优势:
LSTM的门结构能够有效减缓长序列问题中可能出现的梯度消失或爆炸, 虽然并不能杜绝这种现象, 但在更长的序列问题上表现优于传统RNN.
LSTM缺点:
由于内部结构相对较复杂, 因此训练效率在同等算力下较传统RNN低很多.

**GRU模型**

GRU（Gated Recurrent Unit）也称门控循环单元结构, 它也是传统RNN的变体, 同LSTM一样能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时它的结构和计算要比LSTM更简单, 它的核心结构可以分为两个部分去解析:

- 更新门
- 重置门

GRU的内部结构图和计算公式:
![在这里插入图片描述](https://img-blog.csdnimg.cn/91c121517104448b953b87bef3f97045.png)
结构解释图:

![在这里插入图片描述](https://img-blog.csdnimg.cn/784fefed42594a8487113a42abd8a522.png)
GRU的更新门和重置门结构图:
![请添加图片描述](https://img-blog.csdnimg.cn/93784e665ae6499dba2cb31481ec41f4.png)
内部结构分析:
和之前分析过的LSTM中的门控一样, 首先计算更新门和重置门的门值, 分别是z(t)和r(t), 计算方法就是使用X(t)与h(t-1)拼接进行线性变换, 再经过sigmoid激活. 之后更新门门值作用在了h(t-1)上, 代表控制上一时间步传来的信息有多少可以被利用. 接着就是使用这个更新后的h(t-1)进行基本的RNN计算, 即与x(t)拼接进行线性变化, 经过tanh激活, 得到新的h(t). 最后重置门的门值会作用在新的h(t)，而1-门值会作用在h(t-1)上, 随后将两者的结果相加, 得到最终的隐含状态输出h(t), 这个过程意味着重置门有能力重置之前所有的计算, 当门值趋于1时, 输出就是新的h(t), 而当门值趋于0时, 输出就是上一时间步的h(t-1).

Bi-GRU与Bi-LSTM的逻辑相同, 都是不改变其内部结构, 而是将模型应用两次且方向不同, 再将两次得到的LSTM结果进行拼接作为最终输出. 具体参见上小节中的Bi-LSTM.

Pytorch中GRU工具的使用:
位置: 在torch.nn工具包之中, 通过torch.nn.GRU可调用.
nn.GRU类初始化主要参数解释:
input_size: 输入张量x中特征维度的大小.
hidden_size: 隐层张量h中特征维度的大小.
num_layers: 隐含层的数量.
nonlinearity: 激活函数的选择, 默认是tanh.
bidirectional: 是否选择使用双向LSTM, 如果为True, 则使用; 默认不使用.
nn.GRU类实例化对象主要参数解释:
input: 输入张量x.
h0: 初始化的隐层张量h.

```python
>>> import torch
>>> import torch.nn as nn
>>> rnn = nn.GRU(5, 6, 2)
>>> input = torch.randn(1, 3, 5)
>>> h0 = torch.randn(2, 3, 6)
>>> output, hn = rnn(input, h0)
>>> output
tensor([[[-0.2097, -2.2225,  0.6204, -0.1745, -0.1749, -0.0460],
         [-0.3820,  0.0465, -0.4798,  0.6837, -0.7894,  0.5173],
         [-0.0184, -0.2758,  1.2482,  0.5514, -0.9165, -0.6667]]],
       grad_fn=<StackBackward>)
>>> hn
tensor([[[ 0.6578, -0.4226, -0.2129, -0.3785,  0.5070,  0.4338],
         [-0.5072,  0.5948,  0.8083,  0.4618,  0.1629, -0.1591],
         [ 0.2430, -0.4981,  0.3846, -0.4252,  0.7191,  0.5420]],

        [[-0.2097, -2.2225,  0.6204, -0.1745, -0.1749, -0.0460],
         [-0.3820,  0.0465, -0.4798,  0.6837, -0.7894,  0.5173],
         [-0.0184, -0.2758,  1.2482,  0.5514, -0.9165, -0.6667]]],
       grad_fn=<StackBackward>)
```

- GRU的优势:
  GRU和LSTM作用相同, 在捕捉长序列语义关联时, 能有效抑制梯度消失或爆炸, 效果都优于传统RNN且计算复杂度相比LSTM要小.
- GRU的缺点:
  GRU仍然不能完全解决梯度消失问题, 同时其作用RNN的变体, 有着RNN结构本身的一大弊端, 即不可并行计算, 这在数据量和模型体量逐步增大的未来, 是RNN发展的关键瓶颈

### 6.3 应用场景分析

- 古诗生成(一对多任务)
- 情感分析(多对一)
- 股票价格预测

# 第7阶段--人脸识别

### 7.1 人脸识别简介

人脸识别(Face Recognition)是一种依据人的面部特征(如统计或几何特征等)，自动进行身份识别的一种生物识别技术，又称为面像识别、人像识别、相貌识别、面孔识别、面部识别等。通常我们所说的人脸识别是基于光学人脸图像的身份识别与验证的简称。人脸识别利用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸图像进行一系列的相关应用操作。技术上包括图像采集、特征定位、身份的确认和查找等。特征定位，就是从照片中提取人脸中的特征，比如眉毛高度、嘴角等等，再通过特征的对比输出结果。

![](./imgs/face1.png)



### 7.2 人脸识别技术流程简介

主要流程

人脸检测是所有人脸研究的一个前提步骤，它的性能直接影响整个人脸图像应用系统得性能，因此是一个非常关键的步骤。它的任务是首先对由摄像机输入的图像进行分割，即把整幅图像分割成两部：一部分为人脸区域，另一部分为非人脸区域，然后进一步获取脸部信息，并对人脸的行为进行描述，进而完成对人脸识别的分析和理解。

![](./imgs/face2.png)

1. 人脸采集

不同的人脸图像通过摄像镜头采集得到，比如静态图像、动态图像、不同的位置、不同表情等，当采集对象在设备的拍摄范围内时，采集设备会自动搜索并拍摄人脸图像。

人脸采集的主要影响因素：

（1）图像大小：人脸图像过小会影响识别效果，人脸图像过大会影响识别速度。非专业人脸识别摄像头常见规定的最小识别人脸像素为60*60或100*100以上。在规定的图像大小内，算法更容易提升准确率和召回率。图像大小反映在实际应用场景就是人脸离摄像头的距离。

（2）图像分辨率：越低的图像分辨率越难识别。图像大小综合图像分辨率，直接影响摄像头识别距离。现4K摄像头看清人脸的最远距离是10米，7K摄像头是20米。

（3）光照环境：过曝或过暗的光照环境都会影响人脸识别效果。可以从摄像头自带的功能补光或滤光平衡光照影响，也可以利用算法模型优化图像光线

（4）不同遮挡下的人脸识别：遮挡是指正面人脸图像中有眼镜、头发、围巾或者其他的配饰。在过去几年，主要致力于可控设置下的人脸识别；然而，在不可控制下的识别，像光照、表情和部分遮挡是需要考虑的问题

（5）模糊程度：实际场景主要着力解决运动模糊，人脸相对于摄像头的移动经常会产生运动模糊。部分摄像头有抗模糊的功能，而在成本有限的情况下，考虑通过算法模型优化此问题。

（6）遮挡程度：五官无遮挡、脸部边缘清晰的图像为最佳。而在实际场景中，很多人脸都会被帽子、眼镜、口罩等遮挡物遮挡，这部分数据需要根据算法要求决定是否留用训练。

（7）采集角度：人脸相对于摄像头角度为正脸最佳。但实际场景中往往很难抓拍正脸。因此算法模型需训练包含左右侧人脸、上下侧人脸的数据。工业施工上摄像头安置的角度，需满足人脸与摄像头构成的角度在算法识别范围内的要求。

![](./imgs/face3.png)

2. 人脸检测

   在图像中准确标定出人脸的位置和大小，并把其中有用的信息挑出来（如直方图特征、颜色特征、模板特征、结构特征及Haar特征等），然后利用信息来达到人脸检测的目的，常用人脸关键点检测，即自动估计人脸图片上脸部特征点的坐标。

   主流方法：

   ​	基于检测出的特征采用Adaboost学习算法（一种用来分类的方法，它把一些比较弱的分类方法合在一起，组合出新的很强的分类方法）挑选出一些最能代表人脸的矩形特征（弱分类器），按照加权投票的方式将弱分类器构造为一个强分类器，再将训练得到的若干强分类器串联组成一个级联结构的层叠分类器，有效地提高分类器的检测速度。viola-jones框架（性能一般速度尚可，适合移动端、嵌入式上使用），dpm（速度较慢），cnn（性能不错）

   在图像中准确标定出人脸的位置和大小，并把其中有用的信息挑出来（如直方图特征、颜色特征、模板特征、结构特征及Haar特征等），然后利用信息来达到人脸检测的目的，常用人脸关键点检测，即自动估计人脸图片上脸部特征点的坐标。

   ![](./imgs/face5.png)
   
   

### 7.3 人脸识别分类

- Adaboost 人脸检测

  Haar-like 特征可以分为边缘特征、中心特征、对角线特征以及线性特征。

  (a)即为边缘特征，其用于检测目标图像在边缘上的变化信息，如人脸边缘与背景的灰度变化，人头发与人脸额头之间的灰度变化等；

  (b)即为线性特征，其用于检测目标图像在水平以及垂直方向上的变化信息，如人的鼻梁两侧肤色要比鼻梁上的颜色深等；

  (c)即为中心特征和对角线特征，其用于检测对角线上以及矩形模板外围和中心之间的变化信息，如人的眼睛比人脸的其他部分颜色要深，嘴巴要比其周围肤色颜色要深等。并且在这些水平垂直的基础上，又添加了 45°方向的矩形模板。通过采用这三种不同形式的矩形模板来表示人脸区域的特征，达到区分人脸部分与背景部分并将两者分割的目的。 

  ![](./imgs/face6.png)

- 基本原理

  在OpenCV接口中，实现了Haar/LBP/HOG等多种特征，以Haar特征为例介绍,Haar特征最先由Paul Viola等人提出，后经过Rainer Lienhart等扩展引入45°倾斜特征，成为现在OpenCV所使用的的样子共计14种Haar特征，包括5种Basic特征、3种Core特征和6种Titled(即45°旋转)特征。

  人脸检测分类器=haar-like （特征）+CART(弱)+ Adaboost（强） + Cascade（级联）Haar特征可以在检测窗口中由放大+平移产生一系列子特征，但是白：黑区域面积比始终保持不变

  

### 7.4 人脸检测

我们使用机器学习的方法完成人脸检测，首先需要大量的正样本图像（面部图像）和负样本图像（不含面部的图像）来训练分类器。我们需要从其中提取特征。下图中的 Haar 特征会被使用，就像我们的卷积核，每一个特征是一 个值，这个值等于黑色矩形中的像素值之后减去白色矩形中的像素值之和。

Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。

Haar特征可用于于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征。
在这里插入图片描述

![在这里插入图片描述](https://img-blog.csdnimg.cn/d0658b7482504cf7a8fdee89c7b68468.png)

得到图像的特征后，训练一个决策树构建的adaboost级联决策器来识别是否为人脸。

![在这里插入图片描述](https://img-blog.csdnimg.cn/10b020d1c8f643eab1a6a37b2bfe53cc.png)

8.2 实现

OpenCV中自带已训练好的检测器，包括面部，眼睛，猫脸等，都保存在XML文件中，我们可以通过以下程序找到他们：

```python
import cv2 as cv
print(cv.__file__)
```

找到的文件如下所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/3fcb87cb43a049bd9fd23c8e654fdd0e.png)


那我们就利用这些文件来识别人脸，眼睛等。检测流程如下：

1. 读取图片，并转换成灰度图

2. 实例化人脸和眼睛检测的分类器对象

   ```python
   # 实例化级联分类器
   classifier =cv.CascadeClassifier( "haarcascade_frontalface_default.xml" ) 
   # 加载分类器
   classifier.load('haarcascade_frontalface_default.xml')
   ```

3. 进行人脸和眼睛的检测

   ```python
   rect = classifier.detectMultiScale(gray, scaleFactor, minNeighbors, minSize,maxsize) 
   ```

   参数：

   - Gray: 要进行检测的人脸图像
   - scaleFactor: 前后两次扫描中，搜索窗口的比例系数
   - minneighbors：目标至少被检测到minNeighbors次才会被认为是目标
   - minsize和maxsize: 目标的最小尺寸和最大尺寸

4. 将检测结果绘制出来就可以了。

   ![](./imgs/logo.png)

主程序如下所示：

```
# 简化版
import cv2 as cv
img = cv.imread("img_2.png")
print(img.shape)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
print(gray.shape)

# 实例化级联分类器
# print(cv.data.haarcascades)
classifier = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
# # 加载分类器
classifier.load(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cas = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_eye.xml")
eye_cas.load(cv.data.haarcascades + "haarcascade_eye.xml")
faces = classifier.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3,minSize =(32, 32))
for face in faces:
    x, y, w, h = face
    print(x, y, w, h )
    roi_color = img[y:y+h, x:x+w]
    roi_gray = gray[y:y+h, x:x+w]
    eyes = eye_cas.detectMultiScale(roi_gray)
    for (ex, ey, ew, eh) in eyes:
        cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 1)
    cv.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)

    cv.imshow('huizhi', img)
cv.waitKey(0)
cv.destroyAllWindows()

```



```python
import cv2 as cv
import matplotlib.pyplot as plt
# 1.以灰度图的形式读取图片
img = cv.imread("img.png")
print(img.shape)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
print(gray.shape)

# 2.实例化OpenCV人脸和眼睛识别的分类器
face_cas = cv.CascadeClassifier( cv.data.haarcascades + "haarcascade_frontalface_default.xml")
face_cas.load(cv.data.haarcascades + "haarcascade_frontalface_default.xml")

eyes_cas = cv.CascadeClassifier(cv.data.haarcascades +"haarcascade_eye.xml")
eyes_cas.load(cv.data.haarcascades +"haarcascade_eye.xml")

# 3.调用识别人脸
faceRects = face_cas.detectMultiScale( gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))
for faceRect in faceRects:
    x, y, w, h = faceRect
    # 框出人脸
    cv.rectangle(img, (x, y), (x + h, y + w),(0,255,0), 3)
    # 4.在识别出的人脸中进行眼睛的检测
    roi_color = img[y:y+h, x:x+w]
    roi_gray = gray[y:y+h, x:x+w]
    eyes = eyes_cas.detectMultiScale(roi_gray)
    for (ex,ey,ew,eh) in eyes:
        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)
# 5. 检测结果的绘制
plt.figure(figsize=(8,6),dpi=100)
plt.imshow(img[:,:,::-1]),plt.title('res')
plt.xticks([]), plt.yticks([])
plt.show()
```

结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/1dd81ce9be2c40cda15bdada4eb44bc9.png)

我们也可在视频中对人脸进行检测：

```
# 简化版
import cv2 as cv
cap = cv.VideoCapture('timo.mp4')
# 判断

while cap.isOpened():
    # 每一帧
    ret, frame = cap.read()
    if ret == True:
        cv.putText(frame, "timo", (30, 70), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
        classifier = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
        classifier.load(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
        faces = classifier.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))

        for face in faces:
            x, y, w, h = face
            print(x, y, w, h)

            cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0 ), 2)

        cv.imshow("timo", frame)

        # 每一帧间隔25ms
        if cv.waitKey(1) & 0xFF == ord('q'):
            break

cv.release()
cv.destroyAllWindows()
```



```python
import cv2 as cv
import matplotlib.pyplot as plt
# 1.读取视频
cap = cv.VideoCapture("movie.mp4")
# 2.在每一帧数据中进行人脸识别
while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
        # 3.实例化OpenCV人脸识别的分类器 
        face_cas = cv.CascadeClassifier( "haarcascade_frontalface_default.xml" ) 
        face_cas.load('haarcascade_frontalface_default.xml')
        # 4.调用识别人脸 
        faceRects = face_cas.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32)) 
        for faceRect in faceRects: 
            x, y, w, h = faceRect 
            # 框出人脸 
            cv.rectangle(frame, (x, y), (x + h, y + w),(0,255,0), 3) 
        cv.imshow("frame",frame)
        if cv.waitKey(1) & 0xFF == ord('q'):
            break
# 5. 释放资源
cap.release()  
cv.destroyAllWindows()
```

- fer2013人脸表情数据集简介

  fer2013人脸表情数据集由35886张人脸表情图片(48x48)组成，其中，测试图（Training）28708张，公共验证图（PublicTest）和私有验证图（PrivateTest）各3589张，每张图片是由大小固定为48×48的灰度图像组成，共有7种表情，分别对应于数字标签0-6，具体表情对应的标签和中英文如下：0 anger 生气； 1 disgust 厌恶； 2 fear 恐惧； 3 happy 开心； 4 sad 伤心；5 surprised 惊讶； 6 normal 中性。

  但是，数据集并没有直接给出图片，而是将表情、图片数据、用途的数据保存到csv文件中，如下图所示，


![在这里插入图片描述](https://img-blog.csdnimg.cn/01a12dfbf9b943259a5a085e06786125.png)

如上图所示，第一张图是csv文件的开头，第一行是表头，说明每列数据的含义，第一列表示表情标签，第二列即为图片数据，这里是原始的图片数据，最后一列为用途。

```
emotions = {
    '0': 'anger',  # 生气
    '1': 'disgust',  # 厌恶
    '2': 'fear',  # 恐惧
    '3': 'happy',  # 开心
    '4': 'sad',  # 伤心
    '5': 'surprised',  # 惊讶
    '6': 'normal',  # 中性
}
```

```
import numpy as np
import pandas as pd
data = pd.read_csv('fer2013.csv')
from matplotlib import pyplot as plt
print(data.head())
X = []
y = []
for i in range(10):
    temp_y = data.iloc[i, 0]
    temp_X = data.iloc[i, 1]
    ls = list(map(lambda x: int(x), temp_X.split()))
    X.append(np.array(ls).reshape(48, -1))
    y.append(temp_y)
X = np.array(X)
y = np.array(y)
emotions = {
    '0': 'anger',  # 生气
    '1': 'disgust',  # 厌恶
    '2': 'fear',  # 恐惧
    '3': 'happy',  # 开心
    '4': 'sad',  # 伤心
    '5': 'surprised',  # 惊讶
    '6': 'normal',  # 中性
}
plt.figure(figsize=(4, 4))
a = [i for i in range(1,10)]
for i in a:
    plt.subplot(3,3,i)

    plt.imshow(X[i-1])
    plt.title(f'{emotions[str(y[i-1])]}')
    plt.savefig('fer.png')
import cv2 as cv
img = cv.imread("fer.png")
print(img.shape)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
print(gray.shape)

# 实例化级联分类器
# print(cv.data.haarcascades)
classifier = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
# # 加载分类器
classifier.load(cv.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cas = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_eye.xml")
eye_cas.load(cv.data.haarcascades + "haarcascade_eye.xml")
faces = classifier.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3,minSize =(32, 32))
for face in faces:
    x, y, w, h = face
    print(x, y, w, h )
    roi_color = img[y:y+h, x:x+w]
    roi_gray = gray[y:y+h, x:x+w]
    eyes = eye_cas.detectMultiScale(roi_gray)
    for (ex, ey, ew, eh) in eyes:
        cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 1)
    cv.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)

    cv.imshow('huizhi', img)
cv.waitKey(0)
cv.destroyAllWindows()
plt.show()



```





### 7.5 性别和年龄识别

有一组照片，分为男人和女人，让深度学习模型来学习这些样本，并能够找到其中的规律，完成模型的训练。接着可以使用该模型对图片中的人物进行识别，区分其性别是男还是女。使用了一个NASNet_A_Mobile的模型来做二次训练。具体过程分为4步：

（1）准备样本；

（2）准备NASNet_A_Mobile网络模型；

（3）编写代码进行二次训练；

（4）使用已经训练好的模型进行测试。



下载CelebA数据集：

下载完之后，解压，并手动分出一部分男人与女人的照片。在本例中，一共用了20000张图片用来训练模型，其中训练样本由8421张男性头像和11599张女性头像构成（在train文件夹下），测试样本由10张男性头像和10张女性头像构成（在val文件夹下）。部分样本数据如图：

![](./imgs/face7.png)

http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

CelebA是CelebFaces Attribute的缩写，意即名人人脸属性数据集，其包含10,177个名人身份的202,599张人脸图片，每张图片都做好了特征标记，包含人脸bbox标注框、5个人脸特征点坐标以及40个属性标记，CelebA由香港中文大学开放提供，广泛用于人脸相关的计算机视觉训练任务，可用于人脸属性标识训练、人脸检测训练以及landmark标记等，官方为我们提供了如下几个下载链接：

![](./imgs/face8.png)

点击任何一个链接都会进入如下dropbox目录：Anno是bbox、landmark及attribute注释文件，Eval是training、validation及testing数据集的划分注释，Img则是存放相应的人脸图像，README.txt是CelebA介绍文件；

![](./imgs/face9.png)

通过阅读README.txt了解到每一部分代表的含义：- In-The-Wild Images (Img/img_celeba.7z)202,599张原始“野生”人脸图像，从网络爬取未有做任何裁剪缩放操作的人脸图像；- Align&Cropped Images (Img/img_align_celeba.zip & Img/img_align_celeba_png.7z)202,599张经过人脸对齐和裁剪了的图像，视情况下载对应不同质量的图像即可，一般选择jpg格式才1G多的img_align_celeba.zip文件；



CelebA的40个属性的理解：

- 5_o_Clock_Shadow：刚长出的双颊胡须
- Arched_Eyebrows：柳叶眉
- Attractive：吸引人的
- Bags_Under_Eyes：眼袋
- Bald：秃头
- Bangs：刘海
- Big_Lips：大嘴唇
- Big_Nose：大鼻子
- Black_Hair：黑发
- Blond_Hair：金发
- Blurry：模糊的
- Brown_Hair：棕发
- Bushy_Eyebrows：浓眉
- Chubby：圆胖的
- Double_Chin：双下巴
-  Eyeglasses：眼镜
- Goatee：山羊胡子
- Gray_Hair：灰发或白发
- Heavy_Makeup：浓妆
- High_Cheekbones：高颧骨
- Male：男性
- Mouth_Slightly_Open：微微张开嘴巴
- Mustache：胡子，髭
- Narrow_Eyes：细长的眼睛
- No_Beard：无胡子
- Oval_Face：椭圆形的脸
- Pale_Skin：苍白的皮肤
- Pointy_Nose：尖鼻子
- Receding_Hairline：发际线后移
- Rosy_Cheeks：红润的双颊
- Sideburns：连鬓胡子
- Smiling：微笑
- Straight_Hair：直发
-  Wavy_Hair：卷发
- Wearing_Earrings：戴着耳环
- Wearing_Hat：戴着帽子
- Wearing_Lipstick：涂了唇膏
- Wearing_Necklace：戴着项链
- Wearing_Necktie：戴着领带
- Young：年轻人



- Identity Annotations (available upon request)10,177个名人身份标识，图片的序号即是该图片对应的标签；- Evaluation Partitions (Eval/list_eval_partition.txt)用于划分为training，validation及testing等数据集的标签文件，标签0对应training，标签1对应validation，标签2对应testing；

### 7.6 性别和年龄识别代码实现

- 使用 CNN 进行性别和年龄分类

  OpenCV中使用了一个非常简单的卷积神经网络架构，类似于CaffeNet和AlexNet。该网络使用3个卷积层，2个完全连接层和一个最终输出层。下面给出了这些层的详细信息。

Conv1：第一个卷积层有96 个，核大小为 7的卷积核 。
Conv2：第二个卷积层有256 个，核大小为 5的卷积核 。
Conv3：第三个卷积层有 384 个，核大小为 3的卷积核 。
两个全连接层各有 512 个节点。他们使用 Adience 数据集来训练模型

地址 https://talhassner.github.io/home/projects/Adience/Adience-data.html



- 性别预测
  将性别预测定义为一个分类问题。性别预测网络中的输出层是 softmax 类型，有 2 个节点，表示“男”和“女”两类。

- 年龄预测
  理想情况下，年龄预测应该作为回归问题来处理，因为我们期望输出一个实数。然而，使用回归准确估计年龄具有挑战性。即使是人类也无法通过观察一个人来准确预测年龄。但是，我们知道他们是 20 多岁还是 30 多岁。由于这个原因，明智的做法是将这个问题定义为一个分类问题，我们尝试估计这个人所处的年龄组。例如，0-2 岁范围内的年龄是一个类别，4-6 岁是另一个类别类等。Adience 数据集有 8 个类别，分为以下年龄组 [(0 – 2), (4 – 6), (8 – 12), (15 – 20), (25 – 32), (38 – 43), ( 48 – 53), (60 – 100)]。因此，年龄预测网络在最后的 softmax 层中有 8 个节点，表示提到的年龄范围。应该注意的是，从单一图像预测年龄不是一个很容易解决的问题，因为感知的年龄取决于很多因素，相同年龄的人在世界各地可能看起来非常不同。而且，人们非常努力地隐藏他们的真实年龄!

- 代码可以分为四部分：

  1.检测人脸
  2.检测性别
  3.检测年龄
  4.显示输出

- 加载模型

  gender_net.caffemodel：用于性别检测的预训练模型权重, 文件定义了图层参数的内部状态。

  age_net.caffemodel：用于年龄检测的预训练模型权重, 文件定义了图层参数的内部状态。

  opencv_face_detector_uint8.pb  它是一个 protobuf 文件（协议缓冲区），其中包含模型的图形定义和训练权重。这就是我们将用来执行经过训练模型的内容。虽然.pb 文件包含二进制格式的 protobuf，但.pbtxt 文件包含文本格式的 protobuf。包含 TensorFlow 文件

  age_deploy.prototxt 和 gender_deploy.prototxt    .prototxt 文件提供了年龄和性别的网络配置
  
  ![](./imgs/logo.png)

```
# 导包
import cv2 as cv
import math
import time
# 命令行相关
import argparse
# 获得边界框坐标, 也就是人脸在图像中的位置
def getFaceBox(net, frame, conf_threshold=0.7):
    frameOpencvDnn = frame.copy()
    frameHeight = frameOpencvDnn.shape[0]
    frameWidth= frameOpencvDnn.shape[1]
    blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True,False)
    net.setInput(blob)
    detections = net.forward()
    bboxes = []
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > conf_threshold:
            # 提取
            x1 = int(detections[0, 0, i, 3]*frameWidth)
            y1 = int(detections[0, 0, i, 4]*frameHeight)
            x2 = int(detections[0, 0, i, 5]*frameWidth)
            y2 = int(detections[0, 0, i, 6]*frameHeight)
            bboxes.append([x1, y1, x2, y2])
            cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)
    return frameOpencvDnn, bboxes

# 1.检测人脸
parser = argparse.ArgumentParser(description='使用该脚本运行年龄和性别使用OpenCV模型')
parser.add_argument('--input', help='填写测试图片的路径, 跳过测参数将调用电脑相机')
parser.add_argument('--device', default='cpu')
args = parser.parse_args()
# 定义模型路径
faceProto = './conf/opencv_face_detector.pbtxt'
faceModel = './conf/opencv_face_detector_uint8.pb'

ageProto = './conf/age_deploy.prototxt'
ageModel = './conf/age_net.caffemodel'

genderProto = './conf/gender_deploy.prototxt'
genderModel = './conf/gender_net.caffemodel'

# 　设置模型的平均值以及要从中进行分类的年龄组和性别列表。
MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)
ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)' ]
genderList = ['Male', 'Female']
# 加载网络
# 要加载网络，请使用 readNet() 方法。第一个参数用于存储训练权重，第二个参数用于保存网络配置。
ageNet = cv.dnn.readNet(ageModel, ageProto)
genderNet = cv.dnn.readNet(genderModel, genderProto)
faceNet = cv.dnn.readNet(faceModel, faceProto)

# 判断设备
if args.device == 'cpu':
    ageNet.setPreferableBackend(cv.dnn.DNN_TARGET_CPU)
    genderNet.setPreferableBackend(cv.dnn.DNN_TARGET_CPU)
    faceNet.setPreferableBackend(cv.dnn.DNN_TARGET_CPU)
    print("使用的是cpu...")
elif args.device == 'gpu':
    ageNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)
    genderNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)
    faceNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)
    print("使用的是gpu...")
cap = cv.VideoCapture(args.input if args else 0)
while cap.isOpened():
    # 读取frame
    hasFrame, frame = cap.read()
    if not hasFrame:
        cv.waitKey()
        break
    frameFace, bboxes = getFaceBox(faceNet, frame)
    if not bboxes:
        print("没有检测到人脸")
        continue
    padding = 20
    for bbox in bboxes:
        print(bbox)
        face = frame[max(0, bbox[1]-padding):min(bbox[3]+padding, frame.shape[0]-1),
               max(0, bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]
        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)
        # 2.检测性别
        genderNet.setInput(blob)
        genderPreds = genderNet.forward()
        gender = genderList[genderPreds[0].argmax()]
        print(f"性别 : {gender}, conf = {genderPreds[0].max()}")
        # 3.检测年龄
        ageNet.setInput(blob)
        agePreds = ageNet.forward()
        age = ageList[agePreds[0].argmax()]
        print(f"年龄 : {age}, conf = {agePreds[0].max()}")
        # 4.显示输出
        label = f"{gender}: {age}"
        cv.putText(frameFace, label, (bbox[0], bbox[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 1, cv.LINE_AA)

        cv.imshow("Age and Gender", frameFace)
        cv.imwrite(f'outputs-{args.input}', frameFace)

cv.destroyAllWindows()

# python3 AgeGender.py --input sample1.jpg  --device cpu
```

![](./imgs/ress1.png)
